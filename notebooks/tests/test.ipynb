{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Quark\\.conda\\envs\\TFM-env\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning:\n",
      "\n",
      "`should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Common libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime as dt\n",
    "\n",
    "from wind_power_forecasting.nodes  import data_exploration as dexp\n",
    "from wind_power_forecasting.nodes  import data_transformation as dtr\n",
    "from wind_power_forecasting.nodes  import metric\n",
    "from wind_power_forecasting.nodes  import utils\n",
    "\n",
    "# Graphics\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "import plotly as pty\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import cufflinks as cf\n",
    "cf.set_config_file(offline=True)\n",
    "\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Save images \n",
    "DIR = \"../../TFM/reports/figures/\"\n",
    "WF = \"WF1\"\n",
    "IMAGES_PATH = os.path.join(DIR, WF)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "\n",
    "# Ignore warnings (SciPy issue #5998)\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-8-76eb14a2fa16>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-8-76eb14a2fa16>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    dtr.\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "dtr."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algunas pruebas con diferentes modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para establecer un punto de referencia o *benchmark*, probaremos distintos modelos sin profundizar mucho en el tuning de los hiperparámetros. El objetivo es paulatinamente mejorar el rendimiento de los modelos, a base de modificar las fases del proceso (EDA, Feature Engineering, Hyperparameter Tuning, ...).\n",
    "\n",
    "Elegiremos para conformar nuestro set de entrenamiento:\n",
    "* Un solo NWP.\n",
    "* Run de las 00h.\n",
    "* Día D-1.\n",
    "* Variables meteorológicas de partida `time`, `U`, `V` y `T`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Quark\\.conda\\envs\\TFM-env\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning:\n",
      "\n",
      "`should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-12 20:51:34,641 - kedro.io.data_catalog - INFO - Loading data from `X_train_raw` (CSVDataSet)...\n",
      "2020-10-12 20:51:35,048 - kedro.io.data_catalog - INFO - Loading data from `X_test_raw` (CSVDataSet)...\n",
      "2020-10-12 20:51:35,355 - kedro.io.data_catalog - INFO - Loading data from `y_train_raw` (CSVDataSet)...\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "from kedro.io import DataCatalog\n",
    "from kedro.extras.datasets.pandas import CSVDataSet\n",
    "\n",
    "X_train = context.catalog.load('X_train_raw')\n",
    "X_test = context.catalog.load('X_test_raw')\n",
    "Y_train = context.catalog.load('y_train_raw')\n",
    "\n",
    "X_train['Time'] = pd.to_datetime(X_train['Time'], format='%d/%m/%Y %H:%M')\n",
    "X_test['Time'] = pd.to_datetime(X_test['Time'], format='%d/%m/%Y %H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\d\n",
      "\n",
      "<>:3: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\d\n",
      "\n",
      "<>:3: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\d\n",
      "\n",
      "<ipython-input-13-a975f87aabec>:3: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\d\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def input_missing_values(df, cols):\n",
    "    \n",
    "    regex = 'NWP(?P<NWP>\\d{1})_(?P<run>\\d{2}h)_(?P<fc_day>D\\W?\\d?)_(?P<weather_var>\\w{1,4})'\n",
    "    p = re.compile(regex)  \n",
    "    \n",
    "    NWP_met_vars_dict = {\n",
    "        '1': ['U','V','T'],\n",
    "        '2': ['U','V'],\n",
    "        '3': ['U','V','T'],\n",
    "        '4': ['U','V','CLCT']\n",
    "    }\n",
    "    \n",
    "    for col in reversed(cols):\n",
    "        m = p.match(col)\n",
    "        col_name = 'NWP' + m.group('NWP') + '_' +  m.group('run') + '_' + m.group('fc_day') + '_' + m.group('weather_var')\n",
    "\n",
    "        for key, value in NWP_met_vars_dict.items():\n",
    "            for i in value:\n",
    "                if m.group('NWP') == key and m.group('weather_var') == i:\n",
    "                    df['NWP'+ key + '_' + i] = df['NWP'+ key + '_' + i].fillna(df[col_name])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from metpy import calc\n",
    "from metpy.units import units\n",
    "\n",
    "# function to obtain the module of wind velocity\n",
    "get_wind_velmod = lambda x : float(calc.wind_speed(\n",
    "    x.U * units.meter/units.second, \n",
    "    x.V * units.meter/units.second\n",
    ").magnitude)\n",
    "\n",
    "# function to obtain the wind direction\n",
    "get_wind_dir = lambda x : float(calc.wind_direction(\n",
    "    x.U * units.meter/units.second, \n",
    "    x.V * units.meter/units.second, \n",
    "    convention=\"from\"\n",
    ").magnitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operational_analysis.toolkits import filters\n",
    "from operational_analysis.toolkits import power_curve\n",
    "\n",
    "def plot_flagged_pc(ws, p, flag_bool, alpha):\n",
    "    plt.scatter(ws, p, s = 3, alpha = alpha)\n",
    "    plt.scatter(ws[flag_bool], p[flag_bool], s = 3, c = 'red')\n",
    "    plt.xlabel('Wind speed (m/s)')\n",
    "    plt.ylabel('Power (kW)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_by_date(date, X, y):\n",
    "    \"\"\"\n",
    "    It splits X and y sets by a 'Time' value \n",
    "    into sets for training and testing. \n",
    "        - Return: a dictionary with the four sets\n",
    "                  (X_train, y_train, X_test, y_test)\n",
    "    \"\"\"\n",
    "    sets = {}\n",
    "    date_cut = dt.datetime.strptime(date, '%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    X_test = X[X['Time'] > date_cut]\n",
    "    X_train = X[X['Time'] <= date_cut]\n",
    "    y_train = y.loc[X_train.ID - 1]\n",
    "    y_test = y.loc[X_test.ID - 1]\n",
    "    \n",
    "    sets['X_train'] = X_train\n",
    "    sets['X_test'] = X_test\n",
    "    sets['y_train'] = y_train\n",
    "    sets['y_test'] = y_test\n",
    "    \n",
    "    return sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión con KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['True', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'True',\\n       'True',\\n       ...\\n       'True', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'True',\\n       'True'],\\n      dtype='object', length=6239)] are in the [index]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-590fd8e8d818>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    193\u001b[0m     \u001b[0mX_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mtop_stacked\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0msparse_outliers\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mbottom_stacked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m     \u001b[0mX_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mProduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mProduction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mtop_stacked\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0msparse_outliers\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mbottom_stacked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m     \u001b[0mplot_flagged_pc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mProduction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'True'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[1;31m# seleccionar filas correspondientes a los no-outliers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-a4fff90286a7>\u001b[0m in \u001b[0;36mplot_flagged_pc\u001b[1;34m(ws, p, flag_bool, alpha)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mplot_flagged_pc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mws\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflag_bool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mws\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mws\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mflag_bool\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mflag_bool\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'red'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Wind speed (m/s)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Power (kW)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\TFM-env\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    904\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    905\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 906\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    907\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    908\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\TFM-env\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m_get_with\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m         \u001b[1;31m# handle the dup indexing case GH#4246\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 946\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_values_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\TFM-env\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    877\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 879\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    880\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    881\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\TFM-env\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1097\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Cannot index with multidimensional key\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1099\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_iterable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1101\u001b[0m             \u001b[1;31m# nested tuple slicing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\TFM-env\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_iterable\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1035\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1036\u001b[0m         \u001b[1;31m# A collection of keys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1037\u001b[1;33m         \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1038\u001b[0m         return self.obj._reindex_with_indexers(\n\u001b[0;32m   1039\u001b[0m             \u001b[1;33m{\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_dups\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\TFM-env\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[1;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1252\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1254\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_read_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1255\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\TFM-env\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[1;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1296\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmissing\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1297\u001b[0m                 \u001b[0maxis_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1298\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"None of [{key}] are in the [{axis_name}]\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1299\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1300\u001b[0m             \u001b[1;31m# We (temporarily) allow for some missing keys with .loc, except in\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index(['True', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'True',\\n       'True',\\n       ...\\n       'True', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'True',\\n       'True'],\\n      dtype='object', length=6239)] are in the [index]\""
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Automatización de pre-procesado, modelización, validación y prediccion\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor \n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PowerTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.metrics import r2_score, median_absolute_error\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import linear_model, datasets\n",
    "import scipy as sp\n",
    "from operational_analysis.toolkits import filters\n",
    "from operational_analysis.toolkits import power_curve\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.WARN)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def eval_metrics(actual, pred):\n",
    "    rmse = np.sqrt(mean_squared_error(actual, pred))\n",
    "    mae = mean_absolute_error(actual, pred)\n",
    "    r2 = r2_score(actual, pred)\n",
    "    cape_scorer = make_scorer(metric.get_cape, greater_is_better=False)\n",
    "    return rmse, mae, r2, cape\n",
    "\n",
    "\n",
    "# Lista de WFs\n",
    "WF_lst = ['WF1']\n",
    "\n",
    "# lista para guardar los modelos y los scores de cada WF\n",
    "models = []\n",
    "scores_list = []\n",
    "\n",
    "# Data frame para guardar las predicciones (ID, Production)\n",
    "submission_df = pd.DataFrame([], columns=['ID','Production']) \n",
    "\n",
    "# fichero de salida\n",
    "output = open('results.txt', 'a+')\n",
    "\n",
    "for WF in WF_lst:\n",
    "    # Creamos copia de los datos para no perder su formato inicial\n",
    "    X_train_cpy = X_train.copy()\n",
    "    Y_train_cpy = Y_train.copy()\n",
    "    X_test_cpy = X_test.copy()\n",
    "\n",
    "    # Selección de filas por WF\n",
    "    X_train_cpy = X_train_cpy[X_train_cpy['WF'] == WF]\n",
    "    X_test_cpy = X_test_cpy[X_test_cpy['WF'] == WF]\n",
    "\n",
    "    # Identificador de observaciones\n",
    "    ID_train = X_train_cpy['ID']\n",
    "    ID_test = X_test_cpy['ID']\n",
    "\n",
    "    # Seleccion de las filas de Y_train\n",
    "    Y_train_cpy = Y_train_cpy.loc[ID_train.values - 1]\n",
    "\n",
    "    # Pre-procesado de los datos en bruto\n",
    "\n",
    "    # We'll add new columns NWPX_<met_var> without missing values\n",
    "    # new cols: NWP1_U, NWP1_V, NWP1_T, NWP2_U, NWP2_V, NWP3_U, NWP3_V, NWP3_T, NWP4_U, NWP4_V, NWP4_CLCT\n",
    "    \n",
    "    new_cols = ['NWP1_U','NWP1_V','NWP1_T','NWP2_U',\n",
    "                'NWP2_V','NWP3_U','NWP3_V','NWP3_T',\n",
    "                'NWP4_U','NWP4_V','NWP4_CLCT']\n",
    "\n",
    "    def add_new_cols(new_cols, df):\n",
    "        for col in new_cols:\n",
    "            df[col] = np.nan    \n",
    "            \n",
    "    add_new_cols(new_cols, X_train_cpy)\n",
    "    add_new_cols(new_cols, X_test_cpy)\n",
    "    \n",
    "    cols_train = X_train.columns[3:]\n",
    "    cols_test = X_test.columns[3:-9]\n",
    "    X_train_cpy = input_missing_values(X_train_cpy, cols_train)\n",
    "    X_test_cpy = input_missing_values(X_test_cpy, cols_test)\n",
    "    \n",
    "    col_list = ['NWP2_U','NWP2_V','NWP3_U','NWP3_V','NWP3_T']\n",
    "    X_train_cpy.index = X_train_cpy['Time']\n",
    "    \n",
    "    del X_train_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_train_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=2,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "        \n",
    "    X_train_cpy.reset_index(inplace=True)\n",
    "    \n",
    "    X_test_cpy.index = X_test_cpy['Time']\n",
    "    \n",
    "    del X_test_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_test_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=2,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "    \n",
    "        \n",
    "    X_test_cpy.reset_index(inplace=True) \n",
    "    \n",
    "\n",
    "    X_train_cpy['U'] = X_train_cpy.NWP1_U\n",
    "    X_train_cpy['V'] = X_train_cpy.NWP1_V\n",
    "    X_train_cpy['T'] = X_train_cpy.NWP3_T\n",
    "    X_train_cpy['CLCT'] = X_train_cpy.NWP4_CLCT\n",
    "    \n",
    "    X_test_cpy['U'] = X_test_cpy.NWP1_U\n",
    "    X_test_cpy['V'] = X_test_cpy.NWP1_V\n",
    "    X_test_cpy['T'] = X_test_cpy.NWP3_T\n",
    "    X_test_cpy['CLCT'] = X_test_cpy.NWP4_CLCT\n",
    " \n",
    "    X_train_cpy = X_train_cpy[['ID','Time','U','V','T','CLCT']]\n",
    "    X_test_cpy = X_test_cpy[['ID','Time','U','V','T','CLCT']]\n",
    "\n",
    "    ####### Limpieza de outliers ########\n",
    "    X_train_cpy.loc[X_train_cpy['CLCT'] < 0, 'CLCT'] = 0.0\n",
    "    X_test_cpy.loc[X_test_cpy['CLCT'] < 0, 'CLCT'] = 0.0\n",
    "    \n",
    "    # añadir columna Production\n",
    "    X_train_cpy['Production'] = list(Y_train_cpy['Production'])\n",
    "    \n",
    "    # calcular módulo velocidad del viento\n",
    "    X_train_cpy['vel'] = X_train_cpy.apply(get_wind_velmod, axis=1)\n",
    "    \n",
    "    X_ = X_train_cpy[['vel','Production']]\n",
    "    \n",
    "    # parámetros por granja\n",
    "    if WF == 'WF1':\n",
    "        top_frac_max = 0.97\n",
    "        sparse_bin_width = 0.25\n",
    "        frac_std = 0.85\n",
    "        threshold_type = 'scalar'\n",
    "        bottom_max = 6.\n",
    "    elif WF == \"WF2\":\n",
    "        top_frac_max = 0.97\n",
    "        sparse_bin_width = 0.15\n",
    "        frac_std = 0.8\n",
    "        threshold_type = 'scalar'\n",
    "        bottom_max = 5.5\n",
    "    elif WF == \"WF3\":\n",
    "        top_frac_max = 0.97\n",
    "        sparse_bin_width = 0.1\n",
    "        frac_std = 0.8\n",
    "        threshold_type = 'scalar'\n",
    "        bottom_max = 5.5\n",
    "    elif WF == \"WF4\":\n",
    "        top_frac_max = 0.97\n",
    "        sparse_bin_width = 0.1\n",
    "        frac_std = 0.95\n",
    "        threshold_type = 'scalar'\n",
    "        bottom_max = 5.3\n",
    "    elif WF == \"WF5\":\n",
    "        top_frac_max = 0.97\n",
    "        sparse_bin_width = 0.1\n",
    "        frac_std = 0.55\n",
    "        threshold_type = 'scalar'\n",
    "        bottom_max = 5.\n",
    "    else:\n",
    "        top_frac_max = 0.97\n",
    "        sparse_bin_width = 0.05\n",
    "        frac_std = 1.\n",
    "        threshold_type = 'std'\n",
    "        bottom_max = 5.\n",
    "\n",
    "        \n",
    "    # top-curve stacked outliers\n",
    "    top_stacked = filters.window_range_flag(\n",
    "        X_.Production, top_frac_max * X_.Production.max(), X_.Production.max(), X_.vel, 12.5, 2000.\n",
    "    )\n",
    "    \n",
    "    # sparse outliers\n",
    "    max_bin = 0.97*X_['Production'].max()\n",
    "    sparse_outliers = filters.bin_filter(\n",
    "        X_.Production, X_.vel, sparse_bin_width, frac_std * X_.Production.std(), 'median', 0.1, max_bin, threshold_type, 'all')\n",
    "    \n",
    "    # bottom-curve stacked outliers\n",
    "    bottom_stacked = filters.window_range_flag(X_.vel, bottom_max, 40, X_.Production, 0.05, 2000.)\n",
    "    \n",
    "    # deleting outliers\n",
    "    X_.vel = X_.vel[(~top_stacked) & (~sparse_outliers) & (~bottom_stacked)]\n",
    "    X_.Production = X_.Production[(~top_stacked) & (~sparse_outliers) & (~bottom_stacked)]\n",
    "    plot_flagged_pc(X_.vel, X_.Production, np.repeat('True', len(X_.vel)), 0.7)\n",
    "    \n",
    "    # seleccionar filas correspondientes a los no-outliers\n",
    "    X_train_cpy = X_train_cpy.loc[X_train_cpy['vel'].isin(X_.vel)]\n",
    "    Y_train_cpy = Y_train_cpy.loc[Y_train_cpy['ID'].isin(X_train_cpy['ID'])]\n",
    "    Y_train_cpy = Y_train_cpy['Production']\n",
    "\n",
    "    \n",
    "    # Eliminamos las columnas 'vel' y 'Production'\n",
    "    del X_train_cpy['vel']\n",
    "    del X_train_cpy['Production']\n",
    "    \n",
    "    ###################################\n",
    "\n",
    "    # Pre-procesado de los datos\n",
    "    pre_process = ColumnTransformer(remainder = 'passthrough',\n",
    "                                    transformers = [(\n",
    "                                        'drop_columns', 'drop',\n",
    "                                        ['ID','Time','U','V','T','CLCT',\n",
    "                                         'w_dir_sin','w_dir',\n",
    "                                         'hour_cos','hour_sin','hour',\n",
    "                                         'month','month_sin','month_cos'\n",
    "                                        ])\n",
    "                                    ])\n",
    "\n",
    "    # definir pipeline\n",
    "    tt = Pipeline(steps=[\n",
    "        #('powertransformer', PowerTransformer(method='yeo-johnson', standardize=True)),\n",
    "        ('normalization', MinMaxScaler(feature_range=(0, 1))) \n",
    "    ])\n",
    "\n",
    "    prepare_data_pipeline = Pipeline(steps=[\n",
    "        ('attr_adder', dtr.NewFeaturesAdder(add_time_feat=True, add_cycl_feat=True, add_inv_T=False)), \n",
    "        ('pre_processing', pre_process),\n",
    "        ('powertransformer', PowerTransformer(method='yeo-johnson', standardize=True)),\n",
    "        # ('normalization', MinMaxScaler(feature_range=(0, 1)))    \n",
    "    ])\n",
    "    \n",
    "    # apply pipeline\n",
    "    X_train_pped = prepare_data_pipeline.fit_transform(X_train_cpy)\n",
    "    X_test_pped = prepare_data_pipeline.fit_transform(X_test_cpy)\n",
    "    \n",
    "    # create custome scorer: CAPE\n",
    "    cape_scorer = make_scorer(metric.get_cape, greater_is_better=False)\n",
    "    \n",
    "    # Modeling KNN: GridSearch implementation\n",
    "    param_grid = {\n",
    "        'n_neighbors': list(range(1,50,2)),\n",
    "        'algorithm':['auto', 'kd_tree'],\n",
    "        'weights': ['uniform','distance'],\n",
    "        'metric': ['cityblock','mahalanobis','euclidean'],\n",
    "        'p': [1,2]\n",
    "    }\n",
    "    \n",
    "    tscv = TimeSeriesSplit(n_splits=7)\n",
    "    knn_reg = KNeighborsRegressor()\n",
    "    grid_search_knn = GridSearchCV(\n",
    "        knn_reg, \n",
    "        param_grid, \n",
    "        cv= 7,\n",
    "        n_jobs=-1,\n",
    "        scoring=cape_scorer\n",
    "    )\n",
    "\n",
    "    grid_search_knn.fit(X_train_pped, Y_train_cpy)\n",
    "\n",
    "    # Reentrenamos sin validación cruzada utilizando los mejores \n",
    "    # parámetros obtenidos con la validación cruzada\n",
    "\n",
    "    knn_reg2 = KNeighborsRegressor(algorithm=grid_search_knn.best_params_['algorithm'],\n",
    "                                   n_neighbors=grid_search_knn.best_params_['n_neighbors'],\n",
    "                                   weights=grid_search_knn.best_params_['weights'],\n",
    "                                   metric=grid_search_knn.best_params_['metric'],\n",
    "                                   p=grid_search_knn.best_params_['p'])\n",
    "\n",
    "    ttreg = TransformedTargetRegressor(regressor=knn_reg2, \n",
    "                                       transformer=tt, \n",
    "                                       check_inverse=False)\n",
    "\n",
    "    ttreg.fit(X_train_pped, Y_train_cpy)\n",
    "    # knn_reg2.fit(X_train_pped, Y_train_cpy)\n",
    "    \n",
    "    # evaluación sobre el conjunto de test\n",
    "    predictions = ttreg.predict(X_test_pped)\n",
    "\n",
    "    # build prediction matrix (ID,Production)\n",
    "    pred_matrix = np.stack((np.array(ID_test).astype(int), predictions), axis=-1)\n",
    "    df_pred = pd.DataFrame(data=pred_matrix.reshape(-1,2), columns=['ID','Production'])\n",
    "\n",
    "    # añadimos las predicciones al dataframe final que contendrá las de todas las WF\n",
    "    submission_df = submission_df.append(df_pred, ignore_index=True) \n",
    "    print('CAPE:', -grid_search_knn.best_score_)\n",
    "    print('Predictions for {} has been added to submission_df'.format(WF))\n",
    "    print('---------------')\n",
    "    \n",
    "# generamos fichero csv para el submission\n",
    "submission_df.to_csv(\"../../TFM/models/knn_vD.csv\", index=False, sep=\",\") \n",
    "output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión con ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_hidden=1, n_neurons = 30, learning_rate = 3e-3, input_shape=[8]):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    \n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
    "        \n",
    "    model.add(keras.layers.Dense(1))\n",
    "    optimizer = keras.optimizers.SGD(lr = learning_rate)\n",
    "    model.compile(loss = \"mse\", optimizer = optimizer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Automatización de pre-procesado, modelización, validación y prediccion\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor \n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PowerTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from src.functions import metric\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.metrics import r2_score, median_absolute_error\n",
    "import hdbscan\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import linear_model, datasets\n",
    "from sklearn.cluster import DBSCAN\n",
    "import windpowerlib as wp\n",
    "from windpowerlib import power_curves as pc\n",
    "import scipy as sp\n",
    "from operational_analysis.toolkits import filters\n",
    "from operational_analysis.toolkits import power_curve\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.WARN)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def eval_metrics(actual, pred):\n",
    "    rmse = np.sqrt(mean_squared_error(actual, pred))\n",
    "    mae = mean_absolute_error(actual, pred)\n",
    "    r2 = r2_score(actual, pred)\n",
    "    cape_scorer = make_scorer(metric.get_cape, greater_is_better=False)\n",
    "    return rmse, mae, r2, cape\n",
    "\n",
    "\n",
    "# Lista de WFs\n",
    "WF_lst = [1]\n",
    "\n",
    "# lista para guardar los modelos y los scores de cada WF\n",
    "models = []\n",
    "scores_list = []\n",
    "\n",
    "# Data frame para guardar las predicciones (ID, Production)\n",
    "submission_df = pd.DataFrame([], columns=['ID','Production']) \n",
    "\n",
    "for WF in WF_lst:\n",
    "    # Creamos copia de los datos para no perder su formato inicial\n",
    "    X_train_cpy = X_train.copy()\n",
    "    Y_train_cpy = Y_train.copy()\n",
    "    X_test_cpy = X_test.copy()\n",
    "\n",
    "    # Selección de filas por WF\n",
    "    X_train_cpy = X_train_cpy[X_train_cpy['WF'] == WF]\n",
    "    X_test_cpy = X_test_cpy[X_test_cpy['WF'] == WF]\n",
    "\n",
    "    # Identificador de observaciones\n",
    "    ID_train = X_train_cpy['ID']\n",
    "    ID_test = X_test_cpy['ID']\n",
    "\n",
    "    # Seleccion de las filas de Y_train\n",
    "    Y_train_cpy = Y_train_cpy.loc[ID_train.values - 1]\n",
    "\n",
    "    # Pre-procesado de los datos en bruto\n",
    "\n",
    "    # We'll add new columns NWPX_<met_var> without missing values\n",
    "    # new cols: NWP1_U, NWP1_V, NWP1_T, NWP2_U, NWP2_V, NWP3_U, NWP3_V, NWP3_T, NWP4_U, NWP4_V, NWP4_CLCT\n",
    "    \n",
    "    new_cols = ['NWP1_U','NWP1_V','NWP1_T','NWP2_U',\n",
    "                'NWP2_V','NWP3_U','NWP3_V','NWP3_T',\n",
    "                'NWP4_U','NWP4_V','NWP4_CLCT']\n",
    "\n",
    "    def add_new_cols(new_cols, df):\n",
    "        for col in new_cols:\n",
    "            df[col] = np.nan    \n",
    "            \n",
    "    add_new_cols(new_cols, X_train_cpy)\n",
    "    add_new_cols(new_cols, X_test_cpy)\n",
    "    \n",
    "    cols_train = X_train.columns[3:]\n",
    "    cols_test = X_test.columns[3:-9]\n",
    "    X_train_cpy = input_missing_values(X_train_cpy, cols_train)\n",
    "    X_test_cpy = input_missing_values(X_test_cpy, cols_test)\n",
    "    \n",
    "    col_list = ['NWP2_U','NWP2_V','NWP3_U','NWP3_V','NWP3_T']\n",
    "    X_train_cpy.index = X_train_cpy['Time']\n",
    "    \n",
    "    del X_train_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_train_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=2,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "        \n",
    "    X_train_cpy.reset_index(inplace=True)\n",
    "    \n",
    "    X_test_cpy.index = X_test_cpy['Time']\n",
    "    \n",
    "    del X_test_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_test_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=100,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "    \n",
    "        \n",
    "    X_test_cpy.reset_index(inplace=True) \n",
    "    \n",
    "\n",
    "    X_train_cpy['U'] = X_train_cpy.NWP1_U\n",
    "    X_train_cpy['V'] = X_train_cpy.NWP1_V\n",
    "    X_train_cpy['T'] = X_train_cpy.NWP3_T\n",
    "    X_train_cpy['CLCT'] = X_train_cpy.NWP4_CLCT\n",
    "    \n",
    "    X_test_cpy['U'] = X_test_cpy.NWP1_U\n",
    "    X_test_cpy['V'] = X_test_cpy.NWP1_V\n",
    "    X_test_cpy['T'] = X_test_cpy.NWP3_T\n",
    "    X_test_cpy['CLCT'] = X_test_cpy.NWP4_CLCT\n",
    " \n",
    "    X_train_cpy = X_train_cpy[['ID','Time','U','V','T','CLCT']]\n",
    "    X_test_cpy = X_test_cpy[['ID','Time','U','V','T','CLCT']]\n",
    "\n",
    "    ####### Limpieza de outliers ########\n",
    "    X_train_cpy.loc[X_train_cpy['CLCT'] < 0, 'CLCT'] = 0.0\n",
    "    X_test_cpy.loc[X_test_cpy['CLCT'] < 0, 'CLCT'] = 0.0\n",
    "    \n",
    "    # añadir columna Production\n",
    "    X_train_cpy['Production'] = list(Y_train_cpy['Production'])\n",
    "    \n",
    "    # calcular módulo velocidad del viento\n",
    "    X_train_cpy['vel'] = X_train_cpy.apply(get_wind_velmod, axis=1)\n",
    "    \n",
    "    X_ = X_train_cpy[['vel','Production']]\n",
    "    \n",
    "    # parámetros por granja\n",
    "    if WF == 'WF1':\n",
    "        top_frac_max = 0.97\n",
    "        sparse_bin_width = 0.25\n",
    "        frac_std = 0.85\n",
    "        threshold_type = 'scalar'\n",
    "        bottom_max = 6.\n",
    "    elif WF == \"WF2\":\n",
    "        top_frac_max = 0.97\n",
    "        sparse_bin_width = 0.15\n",
    "        frac_std = 0.8\n",
    "        threshold_type = 'scalar'\n",
    "        bottom_max = 5.5\n",
    "    elif WF == \"WF3\":\n",
    "        top_frac_max = 0.97\n",
    "        sparse_bin_width = 0.1\n",
    "        frac_std = 0.8\n",
    "        threshold_type = 'scalar'\n",
    "        bottom_max = 5.5\n",
    "    elif WF == \"WF4\":\n",
    "        top_frac_max = 0.97\n",
    "        sparse_bin_width = 0.1\n",
    "        frac_std = 0.95\n",
    "        threshold_type = 'scalar'\n",
    "        bottom_max = 5.3\n",
    "    elif WF == \"WF5\":\n",
    "        top_frac_max = 0.97\n",
    "        sparse_bin_width = 0.1\n",
    "        frac_std = 0.55\n",
    "        threshold_type = 'scalar'\n",
    "        bottom_max = 5.\n",
    "    else:\n",
    "        top_frac_max = 0.97\n",
    "        sparse_bin_width = 0.05\n",
    "        frac_std = 1.\n",
    "        threshold_type = 'std'\n",
    "        bottom_max = 5.\n",
    "\n",
    "        \n",
    "    # top-curve stacked outliers\n",
    "    top_stacked = filters.window_range_flag(\n",
    "        X_.Production, top_frac_max * X_.Production.max(), X_.Production.max(), X_.vel, 12.5, 2000.\n",
    "    )\n",
    "    \n",
    "    # sparse outliers\n",
    "    max_bin = 0.97*X_['Production'].max()\n",
    "    sparse_outliers = filters.bin_filter(\n",
    "        X_.Production, X_.vel, sparse_bin_width, frac_std * X_.Production.std(), 'median', 0.1, max_bin, threshold_type, 'all')\n",
    "    \n",
    "    # bottom-curve stacked outliers\n",
    "    bottom_stacked = filters.window_range_flag(X_.vel, bottom_max, 40, X_.Production, 0.05, 2000.)\n",
    "    \n",
    "    # deleting outliers\n",
    "    X_.vel = X_.vel[(~top_stacked) & (~sparse_outliers) & (~bottom_stacked)]\n",
    "    X_.Production = X_.Production[(~top_stacked) & (~sparse_outliers) & (~bottom_stacked)]\n",
    "    plot_flagged_pc(X_.vel, X_.Production, np.repeat('True', len(X_.vel)), 0.7)\n",
    "    \n",
    "    # seleccionar filas correspondientes a los no-outliers\n",
    "    X_train_cpy = X_train_cpy.loc[X_train_cpy['vel'].isin(X_.vel)]\n",
    "    Y_train_cpy = Y_train_cpy.loc[Y_train_cpy['ID'].isin(X_train_cpy['ID'])]\n",
    "    Y_train_cpy = Y_train_cpy['Production']\n",
    "\n",
    "    \n",
    "    # Eliminamos las columnas 'vel' y 'Production'\n",
    "    del X_train_cpy['vel']\n",
    "    del X_train_cpy['Production']\n",
    "    \n",
    "    ###################################\n",
    "    \n",
    "    train_test_dfs = split_data_by_date('2018-12-15 23:00:00', X_train_cpy, Y_train_cpy)\n",
    "    X_train_2 = train_test_dfs.get('X_train')\n",
    "    X_val = train_test_dfs.get('X_test')\n",
    "    y_train_2 = train_test_dfs.get('y_train')\n",
    "    y_val = train_test_dfs.get('y_test')\n",
    "    \n",
    "    drop_lst = ['ID','Time','U','V','CLCT']\n",
    "    pre_process = ColumnTransformer(remainder = 'passthrough',\n",
    "                                    transformers = [(\n",
    "                                        'drop_columns', 'drop', drop_lst)\n",
    "                                    ])\n",
    "    prepare_data_pipeline = Pipeline(steps=[\n",
    "        ('attr_adder', dtr.NewFeaturesAdder(add_time_feat=False, add_cycl_feat=False, add_inv_T=False)), \n",
    "        ('pre_processing', pre_process),\n",
    "        ('standard_scaler', StandardScaler())    \n",
    "    ])\n",
    "    \n",
    "    X_train_pped = prepare_data_pipeline.fit_transform(X_train_2)\n",
    "    X_test_pped = prepare_data_pipeline.transform(X_test_cpy)\n",
    "    X_val_pped = prepare_data_pipeline.transform(X_val)\n",
    "    \n",
    "    ann = keras.wrappers.scikit_learn.KerasRegressor(build_model(input_shape=X_train_pped.shape[1]))\n",
    "    \n",
    "    param_grid = {\n",
    "        \"n_hidden\": [1, 3, 5, 7],\n",
    "        \"n_neurons\":[10, 100, 1000],\n",
    "        \"learning_rate\":[1e-6, 1e-4, 1e-2],\n",
    "    }\n",
    "    \n",
    "    n_splits = n_splits\n",
    "    tscv = TimeSeriesSplit(n_splits)\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=tscv, n_jobs=-1, verbose=100,)\n",
    "    \n",
    "    # predicciones sobre el cojunto de validación\n",
    "    val_pred = model.predict(X_val_pped)\n",
    "    \n",
    "    # evaluación sobre el conjunto de test\n",
    "    predictions = model.predict(X_test_pped)\n",
    "\n",
    "    # build prediction matrix (ID,Production)\n",
    "    pred_matrix = np.stack((np.array(ID_test).astype(int), predictions.reshape(-1)), axis=-1)\n",
    "    df_pred = pd.DataFrame(data=pred_matrix.reshape(-1,2), columns=['ID','Production'])\n",
    "\n",
    "    # añadimos las predicciones al dataframe final que contendrá las de todas las WF\n",
    "    submission_df = submission_df.append(df_pred, ignore_index=True) \n",
    "    print('Validation CAPE:', metric.get_cape(y_val, val_pred.reshape(-1)))\n",
    "    print('Predictions for {} has been added to submission_df'.format(WF))\n",
    "    print('---------------')\n",
    "    \n",
    "# generamos fichero csv para el submission\n",
    "submission_df.to_csv(\"../../TFM/models/ANN.csv\", index=False, sep=\",\") \n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argwhere(np.isnan(X_test_pped))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import reciprocal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(hist(reciprocal(1e-1,1e-2,1e-3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(ID_test).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba 1: Regresión polinomial con regularización *ridge*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Automatización de pre-procesado, modelización, validación y prediccion\n",
    "from sklearn.compose import ColumnTransformer \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from src.functions import metric\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import linear_model as lm\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "# Lista de WFs\n",
    "WF_lst = X_train['WF'].unique()\n",
    "\n",
    "# lista para guardar los modelos, uno por WF\n",
    "models = []\n",
    "\n",
    "# Data frame para guardar las predicciones (ID, Production)\n",
    "submission_df = pd.DataFrame([], columns=['ID','Production']) \n",
    "\n",
    "for WF in WF_lst:\n",
    "    # Creamos copia de los datos para no perder su formato inicial\n",
    "    X_train_cpy = X_train.copy()\n",
    "    Y_train_cpy = Y_train.copy()\n",
    "    X_test_cpy = X_test.copy()\n",
    "\n",
    "    # Selección de filas por WF\n",
    "    X_train_cpy = X_train_cpy[X_train_cpy['WF'] == WF]\n",
    "    X_test_cpy = X_test_cpy[X_test_cpy['WF'] == WF]\n",
    "\n",
    "    # Identificador de observaciones\n",
    "    ID_train = X_train_cpy['ID']\n",
    "    ID_test = X_test_cpy['ID']\n",
    "\n",
    "    # Seleccion de las filas de Y_train\n",
    "    Y_train_cpy = Y_train_cpy['Production']\n",
    "    Y_train_cpy = Y_train_cpy.loc[ID_train.values - 1]\n",
    "\n",
    "    # Pre-procesado de los datos en bruto\n",
    "\n",
    "    # We'll add new columns NWPX_<met_var> without missing values\n",
    "    # new cols: NWP1_U, NWP1_V, NWP1_T, NWP2_U, NWP2_V, NWP3_U, NWP3_V, NWP3_T, NWP4_U, NWP4_V, NWP4_CLCT\n",
    "    \n",
    "    new_cols = ['NWP1_U','NWP1_V','NWP1_T','NWP2_U',\n",
    "                'NWP2_V','NWP3_U','NWP3_V','NWP3_T',\n",
    "                'NWP4_U','NWP4_V','NWP4_CLCT']\n",
    "\n",
    "    def add_new_cols(new_cols, df):\n",
    "        for col in new_cols:\n",
    "            df[col] = np.nan    \n",
    "            \n",
    "    add_new_cols(new_cols, X_train_cpy)\n",
    "    add_new_cols(new_cols, X_test_cpy)\n",
    "    \n",
    "    cols_train = X_train.columns[3:]\n",
    "    cols_test = X_test.columns[3:-9]\n",
    "    X_train_cpy = input_missing_values(X_train_cpy, cols_train)\n",
    "    X_test_cpy = input_missing_values(X_test_cpy, cols_test)\n",
    "    \n",
    "    col_list = ['NWP2_U','NWP2_V','NWP3_U','NWP3_V','NWP3_T']\n",
    "    X_train_cpy.index = X_train_cpy['Time']\n",
    "    \n",
    "    del X_train_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_train_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=2,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "\n",
    "    \n",
    "    X_train_cpy.reset_index(inplace=True)\n",
    "\n",
    "    X_test_cpy.index = X_test_cpy['Time']\n",
    "    \n",
    "    del X_test_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_test_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=100,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "    \n",
    "        \n",
    "    X_test_cpy.reset_index(inplace=True) \n",
    "\n",
    "    X_train_cpy['U'] = (X_train_cpy.NWP1_U + X_train_cpy.NWP2_U + X_train_cpy.NWP3_U + X_train_cpy.NWP4_U)/4\n",
    "    X_train_cpy['V'] = (X_train_cpy.NWP1_V + X_train_cpy.NWP2_V + X_train_cpy.NWP3_V + X_train_cpy.NWP4_V)/4\n",
    "    X_train_cpy['T'] = (X_train_cpy.NWP1_T + X_train_cpy.NWP3_T)/2\n",
    "    X_train_cpy['CLCT'] = (X_train_cpy.NWP4_CLCT)\n",
    "    X_train_cpy['T2'] = np.sqrt(X_train_cpy['T'])\n",
    "    \n",
    "    X_test_cpy['U'] = (X_test_cpy.NWP1_U + X_test_cpy.NWP2_U + X_test_cpy.NWP3_U)/3\n",
    "    X_test_cpy['V'] = (X_test_cpy.NWP1_V + X_test_cpy.NWP2_V + X_test_cpy.NWP3_V)/3\n",
    "    X_test_cpy['T'] = (X_test_cpy.NWP1_T + X_test_cpy.NWP3_T)/2\n",
    "    X_test_cpy['CLCT'] = (X_test_cpy.NWP4_CLCT)\n",
    "    X_test_cpy['T2'] = np.sqrt(X_test_cpy['T'])\n",
    "\n",
    "    \n",
    "    \n",
    "    X_train_cpy = X_train_cpy[['Time','U','V','T','CLCT','T2']]\n",
    "    X_test_cpy = X_test_cpy[['Time','U','V','T','CLCT','T2']]\n",
    "    \n",
    "    # Hay 11 valores perdidos en la columna CLCT en X_test_cpy. Los imputamos con la moda\n",
    "    X_test_cpy.fillna(method='bfill', limit=11, inplace=True)\n",
    "\n",
    "\n",
    "    # Pre-procesado de los datos\n",
    "    pre_process = ColumnTransformer(remainder = 'passthrough',\n",
    "                                    transformers = [('drop_columns', 'drop',\n",
    "                                                     ['Time','U','V','T','CLCT','T2'])]\n",
    "    )\n",
    "\n",
    "    # definir pipeline\n",
    "    prepare_data_pipeline = Pipeline(steps=[\n",
    "        ('attr_adder', dtr.DerivedAttributesAdder(add_time_feat=False, add_cyclic_feat=False, add_vel_pot=False)), \n",
    "        ('pre_processing', pre_process),\n",
    "        ('std_scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    # aplicar pipeline\n",
    "    X_train_pped = prepare_data_pipeline.fit_transform(X_train_cpy)\n",
    "\n",
    "    # creamos nuestro scorer basado en el CAPE\n",
    "    cape_scorer = make_scorer(metric.get_cape, greater_is_better=False)\n",
    "\n",
    "    # Modelización: Regresión polinomial utilizando CV con regularización tipo Ride\n",
    "    poly_features = PolynomialFeatures(degree=3, include_bias=False)\n",
    "    X_train_poly = poly_features.fit_transform(X_train_pped)\n",
    "    \n",
    "    rreg = lm.RidgeCV(alphas=np.logspace(-4, -3, 3, 4), store_cv_values=True)\n",
    "    rreg.fit(X_train_poly, Y_train_cpy)\n",
    "    \n",
    "    # guardamos modelo\n",
    "    models.append(joblib.dump(rreg, WF + '_rreg'))\n",
    "\n",
    "    # evaluación sobre el conjunto de test\n",
    "    X_test_pped = prepare_data_pipeline.transform(X_test_cpy)\n",
    "    X_test_poly = poly_features.fit_transform(X_test_pped)\n",
    "    predictions = rreg.predict(X_test_poly)\n",
    "    \n",
    "    # generamos matriz de predicciones (ID,Production)\n",
    "    pred_matrix = np.stack((np.array(ID_test).astype(int), predictions), axis=-1)\n",
    "    df_pred = pd.DataFrame(data=pred_matrix, columns=['ID','Production'])\n",
    "\n",
    "    # añadimos las predicciones al dataframe final que contendrá las de todas las WF\n",
    "    submission_df = submission_df.append(df_pred, ignore_index=True) \n",
    "    print('Predictions for {} has been added to submission_df'.format(WF))\n",
    "    print('RMSE for {} is {}'.format(WF, np.mean(np.sqrt(rreg.cv_values_))))\n",
    "\n",
    "# generamos fichero csv para el submission\n",
    "submission_df.to_csv(\"../../TFM/models/submission_rreg.csv\", index=False, sep=\",\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Y_train_cpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_cpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba 2: Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Automatización de pre-procesado, modelización, validación y prediccion\n",
    "from sklearn.compose import ColumnTransformer \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from src.functions import metric\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_regression\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "\n",
    "# Lista de WFs\n",
    "WF_lst = X_train['WF'].unique()\n",
    "\n",
    "# Data frame para guardar las predicciones (ID, Production)\n",
    "submission_df = pd.DataFrame([], columns=['ID','Production']) \n",
    "\n",
    "for WF in WF_lst:\n",
    "    # Creamos copia de los datos para no perder su formato inicial\n",
    "    X_train_cpy = X_train.copy()\n",
    "    Y_train_cpy = Y_train.copy()\n",
    "    X_test_cpy = X_test.copy()\n",
    "\n",
    "    # Selección de filas por WF\n",
    "    X_train_cpy = X_train_cpy[X_train_cpy['WF'] == WF]\n",
    "    X_test_cpy = X_test_cpy[X_test_cpy['WF'] == WF]\n",
    "\n",
    "    # Identificador de observaciones\n",
    "    ID_train = X_train_cpy['ID']\n",
    "    ID_test = X_test_cpy['ID']\n",
    "\n",
    "    # Seleccion de las filas de Y_train\n",
    "    Y_train_cpy = Y_train_cpy.loc[ID_train.values - 1]\n",
    "\n",
    "    # Pre-procesado de los datos en bruto\n",
    "\n",
    "    # We'll add new columns NWPX_<met_var> without missing values\n",
    "    # new cols: NWP1_U, NWP1_V, NWP1_T, NWP2_U, NWP2_V, NWP3_U, NWP3_V, NWP3_T, NWP4_U, NWP4_V, NWP4_CLCT\n",
    "    \n",
    "    new_cols = ['NWP1_U','NWP1_V','NWP1_T','NWP2_U',\n",
    "                'NWP2_V','NWP3_U','NWP3_V','NWP3_T',\n",
    "                'NWP4_U','NWP4_V','NWP4_CLCT']\n",
    "\n",
    "    def add_new_cols(new_cols, df):\n",
    "        for col in new_cols:\n",
    "            df[col] = np.nan    \n",
    "            \n",
    "    add_new_cols(new_cols, X_train_cpy)\n",
    "    add_new_cols(new_cols, X_test_cpy)\n",
    "    \n",
    "    cols_train = X_train.columns[3:]\n",
    "    cols_test = X_test.columns[3:-9]\n",
    "    X_train_cpy = input_missing_values(X_train_cpy, cols_train)\n",
    "    X_test_cpy = input_missing_values(X_test_cpy, cols_test)\n",
    "    \n",
    "    col_list = ['NWP2_U','NWP2_V','NWP3_U','NWP3_V','NWP3_T']\n",
    "    X_train_cpy.index = X_train_cpy['Time']\n",
    "    \n",
    "    del X_train_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_train_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=2,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "        \n",
    "    X_train_cpy.reset_index(inplace=True)\n",
    "    \n",
    "    X_test_cpy.index = X_test_cpy['Time']\n",
    "    \n",
    "    del X_test_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_test_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=100,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "    \n",
    "        \n",
    "    X_test_cpy.reset_index(inplace=True) \n",
    "    \n",
    "\n",
    "    X_train_cpy['U'] = X_train_cpy.NWP1_U\n",
    "    X_train_cpy['V'] = X_train_cpy.NWP1_V\n",
    "    X_train_cpy['T'] = X_train_cpy.NWP3_T\n",
    "    X_train_cpy['CLCT'] = X_train_cpy.NWP4_CLCT\n",
    "    \n",
    "    X_test_cpy['U'] = X_test_cpy.NWP1_U\n",
    "    X_test_cpy['V'] = X_test_cpy.NWP1_V\n",
    "    X_test_cpy['T'] = X_test_cpy.NWP3_T\n",
    "    X_test_cpy['CLCT'] = X_test_cpy.NWP4_CLCT\n",
    " \n",
    "    X_train_cpy = X_train_cpy[['ID','Time','U','V','T','CLCT']]\n",
    "    X_test_cpy = X_test_cpy[['ID','Time','U','V','T','CLCT']]\n",
    "\n",
    "    ####### Limpieza de outliers ########\n",
    "    X_train_cpy.loc[X_train_cpy['CLCT'] < 0, 'CLCT'] = 0.0\n",
    "    X_test_cpy.loc[X_test_cpy['CLCT'] < 0, 'CLCT'] = 0.0\n",
    "    \n",
    "    # añadir columna Production\n",
    "    X_train_cpy['Production'] = list(Y_train_cpy['Production'])\n",
    "    \n",
    "    # calcular módulo velocidad del viento\n",
    "    X_train_cpy['vel'] = X_train_cpy.apply(get_wind_velmod, axis=1)\n",
    "    \n",
    "    X_ = X_train_cpy[['vel','Production']]\n",
    "    \n",
    "    # parámetros por granja\n",
    "    if WF == 'WF1':\n",
    "        top_frac_max = 0.97\n",
    "        sparse_bin_width = 0.25\n",
    "        frac_std = 0.85\n",
    "        threshold_type = 'scalar'\n",
    "        bottom_max = 6.\n",
    "    elif WF == \"WF2\":\n",
    "        top_frac_max = 0.97\n",
    "        sparse_bin_width = 0.15\n",
    "        frac_std = 0.8\n",
    "        threshold_type = 'scalar'\n",
    "        bottom_max = 5.5\n",
    "    elif WF == \"WF3\":\n",
    "        top_frac_max = 0.97\n",
    "        sparse_bin_width = 0.1\n",
    "        frac_std = 0.8\n",
    "        threshold_type = 'scalar'\n",
    "        bottom_max = 5.5\n",
    "    elif WF == \"WF4\":\n",
    "        top_frac_max = 0.97\n",
    "        sparse_bin_width = 0.1\n",
    "        frac_std = 0.95\n",
    "        threshold_type = 'scalar'\n",
    "        bottom_max = 5.3\n",
    "    elif WF == \"WF5\":\n",
    "        top_frac_max = 0.97\n",
    "        sparse_bin_width = 0.1\n",
    "        frac_std = 0.55\n",
    "        threshold_type = 'scalar'\n",
    "        bottom_max = 5.\n",
    "    else:\n",
    "        top_frac_max = 0.97\n",
    "        sparse_bin_width = 0.05\n",
    "        frac_std = 1.\n",
    "        threshold_type = 'std'\n",
    "        bottom_max = 5.\n",
    "\n",
    "        \n",
    "    # top-curve stacked outliers\n",
    "    top_stacked = filters.window_range_flag(\n",
    "        X_.Production, top_frac_max * X_.Production.max(), X_.Production.max(), X_.vel, 12.5, 2000.\n",
    "    )\n",
    "    \n",
    "    # sparse outliers\n",
    "    max_bin = 0.97*X_['Production'].max()\n",
    "    sparse_outliers = filters.bin_filter(\n",
    "        X_.Production, X_.vel, sparse_bin_width, frac_std * X_.Production.std(), 'median', 0.1, max_bin, threshold_type, 'all')\n",
    "    \n",
    "    # bottom-curve stacked outliers\n",
    "    bottom_stacked = filters.window_range_flag(X_.vel, bottom_max, 40, X_.Production, 0.05, 2000.)\n",
    "    \n",
    "    # deleting outliers\n",
    "    X_.vel = X_.vel[(~top_stacked) & (~sparse_outliers) & (~bottom_stacked)]\n",
    "    X_.Production = X_.Production[(~top_stacked) & (~sparse_outliers) & (~bottom_stacked)]\n",
    "    plot_flagged_pc(X_.vel, X_.Production, np.repeat('True', len(X_.vel)), 0.7)\n",
    "    \n",
    "    # seleccionar filas correspondientes a los no-outliers\n",
    "    X_train_cpy = X_train_cpy.loc[X_train_cpy['vel'].isin(X_.vel)]\n",
    "    Y_train_cpy = Y_train_cpy.loc[Y_train_cpy['ID'].isin(X_train_cpy['ID'])]\n",
    "    Y_train_cpy = Y_train_cpy['Production']\n",
    "\n",
    "    \n",
    "    # Eliminamos las columnas 'vel' y 'Production'\n",
    "    del X_train_cpy['vel']\n",
    "    del X_train_cpy['Production']\n",
    "    \n",
    "    ###################################\n",
    "\n",
    "    # Pre-procesado de los datos\n",
    "    drop_lst = ['ID','Time','U','V','T','CLCT']\n",
    "    pre_process = ColumnTransformer(remainder = 'passthrough',\n",
    "                                    transformers = [('drop_columns', 'drop', drop_lst)])\n",
    "\n",
    "    # definir pipeline\n",
    "    prepare_data_pipeline = Pipeline(steps=[\n",
    "        ('attr_adder', dtr.NewFeaturesAdder(add_time_feat=False, add_cycl_feat=False, add_inv_T=False)), \n",
    "        ('pre_processing', pre_process),\n",
    "        # ('power_transf', PowerTransformer(method='yeo-johnson', standardize=True))\n",
    "    ])\n",
    "\n",
    "    # aplicar pipeline\n",
    "    X_train_pped = prepare_data_pipeline.fit_transform(X_train_cpy)\n",
    "    X_test_pped = prepare_data_pipeline.transform(X_test_cpy)\n",
    "\n",
    "    # feature_sel = True\n",
    "    # fe# ature_names = X_train_cpy.drop(drop_lst, axis=1).columns\n",
    "    # \n",
    "    # # Feature selection \n",
    "    # if feature_sel: \n",
    "    #     ## using Mutual Information\n",
    "    #     selec_k_best = SelectKBest(mutual_info_regression, k=7)\n",
    "    #     selec_k_best.fit(X_train_pped, Y_train_cpy)\n",
    "    #     X_train_pped = selec_k_best.transform(X_train_pped)\n",
    "    #     X_test_pped = selec_k_best.transform(X_test_pped)\n",
    "    # \n",
    "    #     mask =selec_k_best.get_support() #list of booleans\n",
    "    #     selected_feat = [] \n",
    "    # \n",
    "    #     for bool, feature in zip(mask, feature_names):\n",
    "    #         if bool:\n",
    "    #             selected_feat.append(feature) \n",
    "\n",
    "    # creamos nuestro scorer basado en el CAPE\n",
    "    cape_scorer = make_scorer(metric.get_cape, greater_is_better=False)\n",
    "\n",
    "    param_grid = {\n",
    "        'n_estimators': [75, 150, 225],\n",
    "        'max_features': ['sqrt', 'log2', 'auto'],\n",
    "        'max_depth': [10, 50, 100],\n",
    "        'min_samples_split': [15, 30, 45],\n",
    "        'min_samples_leaf': [4, 6, 8],\n",
    "    }\n",
    "    \n",
    "    \n",
    "    n_splits = 7 \n",
    "    forest_reg = RandomForestRegressor(bootstrap=True, random_state=42)\n",
    "    grid_search = GridSearchCV(\n",
    "        forest_reg, \n",
    "        param_grid, \n",
    "        cv=TimeSeriesSplit(n_splits),\n",
    "        scoring=cape_scorer,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    grid_search.fit(X_train_pped, Y_train_cpy)\n",
    "    final_model = grid_search.best_estimator_\n",
    "    final_model.fit(X_train_pped, Y_train_cpy)\n",
    "\n",
    "    # evaluación sobre el conjunto de test\n",
    "    predictions = final_model.predict(X_test_pped)\n",
    "\n",
    "    # generamos matriz de predicciones (ID,Production)\n",
    "    pred_matrix = np.stack((np.array(ID_test).astype(int), predictions), axis=-1)\n",
    "    df_pred = pd.DataFrame(data=pred_matrix, columns=['ID','Production'])\n",
    "\n",
    "    # añadimos las predicciones al dataframe final que contendrá las de todas las WF\n",
    "    submission_df = submission_df.append(df_pred, ignore_index=True)  \n",
    "    print('Best CV score for {}: {}'.format(WF, -grid_search.best_score_))\n",
    "    print('Predictions for {} has been added to submission_df'.format(WF))\n",
    "    print('--------')\n",
    "\n",
    "# generamos fichero csv para el submission\n",
    "submission_df.to_csv(\"../../TFM/models/submission_RF.csv\", index=False, sep=\",\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba 3: SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Automatización de pre-procesado, modelización, validación y prediccion\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from src.functions import metric\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_regression\n",
    "import hdbscan\n",
    "\n",
    "# Lista de WFs\n",
    "WF_lst = X_train['WF'].unique()\n",
    "\n",
    "# lista para guardar los modelos, uno por WF\n",
    "models = []\n",
    "scores_list = []\n",
    "\n",
    "# Data frame para guardar las predicciones (ID, Production)\n",
    "submission_df = pd.DataFrame([], columns=['ID','Production']) \n",
    "\n",
    "\n",
    "\n",
    "for WF in WF_lst:\n",
    "    # Creamos copia de los datos para no perder su formato inicial\n",
    "    X_train_cpy = X_train.copy()\n",
    "    Y_train_cpy = Y_train.copy()\n",
    "    X_test_cpy = X_test.copy()\n",
    "\n",
    "    # Selección de filas por WF\n",
    "    X_train_cpy = X_train_cpy[X_train_cpy['WF'] == WF]\n",
    "    X_test_cpy = X_test_cpy[X_test_cpy['WF'] == WF]\n",
    "\n",
    "    # Identificador de observaciones\n",
    "    ID_train = X_train_cpy['ID']\n",
    "    ID_test = X_test_cpy['ID']\n",
    "\n",
    "    # Seleccion de las filas de Y_train\n",
    "    Y_train_cpy = Y_train_cpy['Production']\n",
    "    Y_train_cpy = Y_train_cpy.loc[ID_train.values - 1]\n",
    "\n",
    "    # Pre-procesado de los datos en bruto\n",
    "\n",
    "    # We'll add new columns NWPX_<met_var> without missing values\n",
    "    # new cols: NWP1_U, NWP1_V, NWP1_T, NWP2_U, NWP2_V, NWP3_U, NWP3_V, NWP3_T, NWP4_U, NWP4_V, NWP4_CLCT\n",
    "    \n",
    "    new_cols = ['NWP1_U','NWP1_V','NWP1_T','NWP2_U',\n",
    "                'NWP2_V','NWP3_U','NWP3_V','NWP3_T',\n",
    "                'NWP4_U','NWP4_V','NWP4_CLCT']\n",
    "\n",
    "    def add_new_cols(new_cols, df):\n",
    "        for col in new_cols:\n",
    "            df[col] = np.nan    \n",
    "            \n",
    "    add_new_cols(new_cols, X_train_cpy)\n",
    "    add_new_cols(new_cols, X_test_cpy)\n",
    "    \n",
    "    cols_train = X_train.columns[3:]\n",
    "    cols_test = X_test.columns[3:-9]\n",
    "    X_train_cpy = input_missing_values(X_train_cpy, cols_train)\n",
    "    X_test_cpy = input_missing_values(X_test_cpy, cols_test)\n",
    "    \n",
    "    col_list = ['NWP2_U','NWP2_V','NWP3_U','NWP3_V','NWP3_T']\n",
    "    X_train_cpy.index = X_train_cpy['Time']\n",
    "    \n",
    "    del X_train_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_train_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=2,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "\n",
    "    \n",
    "    X_train_cpy.reset_index(inplace=True)\n",
    "\n",
    "    X_test_cpy.index = X_test_cpy['Time']\n",
    "    \n",
    "    del X_test_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_test_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=100,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "    \n",
    "        \n",
    "    X_test_cpy.reset_index(inplace=True) \n",
    "\n",
    "    X_train_cpy['U'] = X_train_cpy.NWP1_U\n",
    "    X_train_cpy['V'] = X_train_cpy.NWP1_V\n",
    "    X_train_cpy['T'] = X_train_cpy.NWP3_T\n",
    "    X_train_cpy['CLCT'] = X_train_cpy.NWP4_CLCT\n",
    "    \n",
    "    X_test_cpy['U'] = X_test_cpy.NWP1_U\n",
    "    X_test_cpy['V'] = X_test_cpy.NWP1_V\n",
    "    X_test_cpy['T'] = X_test_cpy.NWP3_T\n",
    "    X_test_cpy['CLCT'] = X_test_cpy.NWP4_CLCT\n",
    " \n",
    "    X_train_cpy = X_train_cpy[['ID','Time','U','V','T','CLCT']]\n",
    "    X_test_cpy = X_test_cpy[['ID','Time','U','V','T','CLCT']]\n",
    "    \n",
    "    # Hay 11 valores perdidos en la columna CLCT en X_test_cpy. Los imputamos con la moda\n",
    "    X_test_cpy.fillna(method='bfill', limit=11, inplace=True)\n",
    "    \n",
    "    \n",
    "    # valores negativos en CLCT\n",
    "    X_train_cpy.loc[X_train_cpy['CLCT'] < 0, 'CLCT'] = 0.0\n",
    "    X_test_cpy.loc[X_test_cpy['CLCT'] < 0, 'CLCT'] = 0.0\n",
    "\n",
    "    \n",
    "    # Pre-procesado de los datos\n",
    "    pre_process = ColumnTransformer(remainder = 'passthrough',\n",
    "                                    transformers = [('drop_columns', 'drop', \n",
    "                                                     ['ID','Time','U','V','CLCT','month'])]\n",
    "    )\n",
    "\n",
    "    # definir pipeline\n",
    "    prepare_data_pipeline = Pipeline(steps=[\n",
    "        ('attr_adder', dtr.NewFeaturesAdder(add_inv_T=False, add_cycl_feat=False)), \n",
    "        ('pre_processing', pre_process),\n",
    "        ('std_scaler', PowerTransformer(method='yeo-johnson', standardize=True))\n",
    "    ])\n",
    "\n",
    "    # aplicar pipeline\n",
    "    X_train_pped = prepare_data_pipeline.fit_transform(X_train_cpy)\n",
    "    \n",
    "    ####### Limpieza de outliers ########\n",
    "    \n",
    "    # añadir columna Production\n",
    "    X_train_cpy['Production'] = Y_train_cpy.to_list()\n",
    "    \n",
    "    # calcular módulo velocidad del viento\n",
    "    X_train_cpy['vel'] = X_train_cpy.apply(get_wind_velmod, axis=1)\n",
    "    \n",
    "    # formar matriz de datos \n",
    "    scaler = StandardScaler()\n",
    "    X1 = scaler.fit_transform(np.array(X_train_cpy['vel']).reshape(-1,1))\n",
    "    X2 = scaler.fit_transform(np.array(X_train_cpy['Production']).reshape(-1,1))\n",
    "    X = np.concatenate((X1,X2), axis=1)\n",
    "    \n",
    "    # algoritmo para detección de outliers\n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=20, metric='mahalanobis', algorithm='brute').fit(X)\n",
    "    threshold = pd.Series(clusterer.outlier_scores_).quantile(0.95)\n",
    "    outliers = np.where(clusterer.outlier_scores_ > threshold)[0]\n",
    "    \n",
    "    # Eliminamos los registros outliers \n",
    "    X_train_pped = np.delete(X_train_pped, outliers, axis=0)\n",
    "    \n",
    "    # Eliminamos las observaciones corresp#ondientes de Y_train\n",
    "    Y_train_cpy = np.delete(X2, outliers, axis=0)\n",
    "    \n",
    "    # Eliminamos las columnas 'vel' y 'Production'\n",
    "    del X_train_cpy['vel']\n",
    "    del X_train_cpy['Production']\n",
    "    \n",
    "    ###################################\n",
    "    \n",
    "    '''\n",
    "    print('Nº features antes de la seleccion: ', X_train_pped.shape[1])\n",
    "    # seleccionar features más importantes mediante Random Forest\n",
    "    rf = RandomForestRegressor(random_state=42)\n",
    "    rf_reg = rf.fit(X_train_pped, Y_train_cpy)\n",
    "    sel = SelectFromModel(rf_reg, prefit=True, threshold='1.75*median')\n",
    "    X_train_pped = sel.transform(X_train_pped)\n",
    "    \n",
    "    print('Nº features después de la seleccion: ', X_train_pped.shape[1])\n",
    "    '''\n",
    "  \n",
    "\n",
    "    # creamos nuestro scorer basado en el CAPE\n",
    "    cape_scorer = make_scorer(metric.get_cape, greater_is_better=False)\n",
    "\n",
    "    # Entrenamiento del modelo mediante k-fold cross validation\n",
    "    # Búsqueda de hiperparámetros mediante Gridsearch \n",
    "    param_grid = {\n",
    "        'kernel': ('linear', 'rbf','poly'), \n",
    "        'C':[0.01, 1, 0.1, 10],\n",
    "        'gamma': [0.00001, 0.001, 1],\n",
    "        'epsilon':[0.1,0.3,0.5]\n",
    "    }\n",
    "\n",
    "    svm_reg = SVR()\n",
    "    grid_search_svm = GridSearchCV(\n",
    "        svm_reg, \n",
    "        param_grid, \n",
    "        cv= TimeSeriesSplit(n_splits=7),\n",
    "        scoring=cape_scorer,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    grid_search_svm.fit(X_train_pped, Y_train_cpy)\n",
    "\n",
    "    # Reentrenamos sin validación cruzada utilizando los mejores \n",
    "    # parámetros obtenidos con la validación cruzada\n",
    "    \n",
    "    svm_reg2 = SVR(kernel = grid_search_svm.best_params_['kernel'],\n",
    "                  C = grid_search_svm.best_params_['C'],\n",
    "                  gamma = grid_search_svm.best_params_['gamma'],\n",
    "                  epsilon = grid_search_svm.best_params_['epsilon'])\n",
    "    \n",
    "    ttreg = TransformedTargetRegressor(regressor=svm_reg2, \n",
    "                                       transformer=PowerTransformer(method='yeo-johnson', standardize=True), \n",
    "                                       check_inverse=False)\n",
    "\n",
    "    ttreg.fit(X_train_pped, Y_train_cpy)\n",
    "\n",
    "    # evaluación sobre el conjunto de test\n",
    "    X_test_pped = prepare_data_pipeline.transform(X_test_cpy)\n",
    "    # X_test_pped = sel.transform(X_test_pped)\n",
    "    predictions = ttreg.predict(X_test_pped)\n",
    "    \n",
    "    # generamos matriz de predicciones (ID,Production)\n",
    "    pred_matrix = np.stack((np.array(ID_test).astype(int), predictions), axis=-1)\n",
    "    df_pred = pd.DataFrame(data=pred_matrix, columns=['ID','Production'])\n",
    "\n",
    "    # añadimos las predicciones al dataframe final que contendrá las de todas las WF\n",
    "    submission_df = submission_df.append(df_pred, ignore_index=True) \n",
    "    print('R2 for {} is {}'.format(WF, ttreg.score(X_train_pped, Y_train_cpy)))\n",
    "    print('Predictions for {} has been added to submission_df'.format(WF))\n",
    "    \n",
    "    scores_list.append(ttreg.score(X_train_pped, Y_train_cpy))\n",
    "    \n",
    "global_score = np.mean(scores_list)\n",
    "\n",
    "print('********************************************')\n",
    "print('Global score: ', global_score)\n",
    "print('********************************************')\n",
    "\n",
    "\n",
    "# generamos fichero csv para el submission\n",
    "submission_df.to_csv(\"../../TFM/models/submission_SVR.csv\", index=False, sep=\",\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None,\n",
    "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"\n",
    "    Generate 3 plots: the test and training learning curve, the training\n",
    "    samples vs fit times curve, the fit times vs score curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "        An object of that type which is cloned for each validation.\n",
    "\n",
    "    title : string\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "        Target relative to X for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    axes : array of 3 axes, optional (default=None)\n",
    "        Axes to use for plotting the curves.\n",
    "\n",
    "    ylim : tuple, shape (ymin, ymax), optional\n",
    "        Defines minimum and maximum yvalues plotted.\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, optional\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "          - None, to use the default 5-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - :term:`CV splitter`,\n",
    "          - An iterable yielding (train, test) splits as arrays of indices.\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validators that can be used here.\n",
    "\n",
    "    n_jobs : int or None, optional (default=None)\n",
    "        Number of jobs to run in parallel.\n",
    "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
    "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
    "        for more details.\n",
    "\n",
    "    train_sizes : array-like, shape (n_ticks,), dtype float or int\n",
    "        Relative or absolute numbers of training examples that will be used to\n",
    "        generate the learning curve. If the dtype is float, it is regarded as a\n",
    "        fraction of the maximum size of the training set (that is determined\n",
    "        by the selected validation method), i.e. it has to be within (0, 1].\n",
    "        Otherwise it is interpreted as absolute sizes of the training sets.\n",
    "        Note that for classification the number of samples usually have to\n",
    "        be big enough to contain at least one sample from each class.\n",
    "        (default: np.linspace(0.1, 1.0, 5))\n",
    "    \"\"\"\n",
    "    if axes is None:\n",
    "        _, axes = plt.subplots(1,3, figsize=(20, 5))\n",
    "\n",
    "    axes[0].set_title(title)\n",
    "    if ylim is not None:\n",
    "        axes[0].set_ylim(*ylim)\n",
    "    axes[0].set_xlabel(\"Training examples\")\n",
    "    axes[0].set_ylabel(\"Score\")\n",
    "\n",
    "    train_sizes, train_scores, test_scores, fit_times, _ = \\\n",
    "        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n",
    "                       train_sizes=train_sizes,\n",
    "                       return_times=True)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    fit_times_mean = np.mean(fit_times, axis=1)\n",
    "    fit_times_std = np.std(fit_times, axis=1)\n",
    "\n",
    "    # Plot learning curve\n",
    "    axes[0].grid()\n",
    "    axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                         train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                         color=\"r\")\n",
    "    axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                         test_scores_mean + test_scores_std, alpha=0.1,\n",
    "                         color=\"g\")\n",
    "    axes[0].plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "                 label=\"Training score\")\n",
    "    axes[0].plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "                 label=\"Cross-validation score\")\n",
    "    axes[0].legend(loc=\"best\")\n",
    "\n",
    "    # Plot n_samples vs fit_times\n",
    "    axes[1].grid()\n",
    "    axes[1].plot(train_sizes, fit_times_mean, 'o-')\n",
    "    axes[1].fill_between(train_sizes, fit_times_mean - fit_times_std,\n",
    "                         fit_times_mean + fit_times_std, alpha=0.1)\n",
    "    axes[1].set_xlabel(\"Training examples\")\n",
    "    axes[1].set_ylabel(\"fit_times\")\n",
    "    axes[1].set_title(\"Scalability of the model\")\n",
    "\n",
    "    # Plot fit_time vs score\n",
    "    axes[2].grid()\n",
    "    axes[2].plot(fit_times_mean, test_scores_mean, 'o-')\n",
    "    axes[2].fill_between(fit_times_mean, test_scores_mean - test_scores_std,\n",
    "                         test_scores_mean + test_scores_std, alpha=0.1)\n",
    "    axes[2].set_xlabel(\"fit_times\")\n",
    "    axes[2].set_ylabel(\"Score\")\n",
    "    axes[2].set_title(\"Performance of the model\")\n",
    "\n",
    "    return plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba 4: XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Automatización de pre-procesado, modelización, validación y prediccion\n",
    "from sklearn.compose import ColumnTransformer \n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from src.functions import metric\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit, learning_curve\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import hdbscan\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "# Lista de WFs\n",
    "WF_lst = X_train['WF'].unique()\n",
    "\n",
    "# lista para guardar los modelos, uno por WF\n",
    "models = []\n",
    "scores_list = []\n",
    "\n",
    "# Data frame para guardar las predicciones (ID, Production)\n",
    "submission_df = pd.DataFrame([], columns=['ID','Production']) \n",
    "\n",
    "for WF in WF_lst:\n",
    "    # Creamos copia de los datos para no perder su formato inicial\n",
    "    X_train_cpy = X_train.copy()\n",
    "    Y_train_cpy = Y_train.copy()\n",
    "    X_test_cpy = X_test.copy()\n",
    "\n",
    "    # Selección de filas por WF\n",
    "    X_train_cpy = X_train_cpy[X_train_cpy['WF'] == WF]\n",
    "    X_test_cpy = X_test_cpy[X_test_cpy['WF'] == WF]\n",
    "\n",
    "    # Identificador de observaciones\n",
    "    ID_train = X_train_cpy['ID']\n",
    "    ID_test = X_test_cpy['ID']\n",
    "\n",
    "    # Seleccion de las filas de Y_train\n",
    "    Y_train_cpy = Y_train_cpy['Production']\n",
    "    Y_train_cpy = Y_train_cpy.loc[ID_train.values - 1]\n",
    "\n",
    "    # Pre-procesado de los datos en bruto\n",
    "\n",
    "    # We'll add new columns NWPX_<met_var> without missing values\n",
    "    # new cols: NWP1_U, NWP1_V, NWP1_T, NWP2_U, NWP2_V, NWP3_U, NWP3_V, NWP3_T, NWP4_U, NWP4_V, NWP4_CLCT\n",
    "    \n",
    "    new_cols = ['NWP1_U','NWP1_V','NWP1_T','NWP2_U',\n",
    "                'NWP2_V','NWP3_U','NWP3_V','NWP3_T',\n",
    "                'NWP4_U','NWP4_V','NWP4_CLCT']\n",
    "\n",
    "    def add_new_cols(new_cols, df):\n",
    "        for col in new_cols:\n",
    "            df[col] = np.nan    \n",
    "            \n",
    "    add_new_cols(new_cols, X_train_cpy)\n",
    "    add_new_cols(new_cols, X_test_cpy)\n",
    "    \n",
    "    cols_train = X_train.columns[3:]\n",
    "    cols_test = X_test.columns[3:-9]\n",
    "    X_train_cpy = input_missing_values(X_train_cpy, cols_train)\n",
    "    X_test_cpy = input_missing_values(X_test_cpy, cols_test)\n",
    "    \n",
    "    col_list = ['NWP2_U','NWP2_V','NWP3_U','NWP3_V','NWP3_T']\n",
    "    X_train_cpy.index = X_train_cpy['Time']\n",
    "    \n",
    "    del X_train_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_train_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=2,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "\n",
    "    \n",
    "    X_train_cpy.reset_index(inplace=True)\n",
    "\n",
    "    X_test_cpy.index = X_test_cpy['Time']\n",
    "    \n",
    "    del X_test_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_test_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=100,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "    \n",
    "        \n",
    "    X_test_cpy.reset_index(inplace=True) \n",
    "\n",
    "    X_train_cpy['U'] = (X_train_cpy.NWP1_U + X_train_cpy.NWP2_U + X_train_cpy.NWP3_U + X_train_cpy.NWP4_U)/4\n",
    "    X_train_cpy['V'] = (X_train_cpy.NWP1_V + X_train_cpy.NWP2_V + X_train_cpy.NWP3_V + X_train_cpy.NWP4_V)/4\n",
    "    X_train_cpy['T'] = (X_train_cpy.NWP1_T + X_train_cpy.NWP3_T)/2\n",
    "    X_train_cpy['CLCT'] = (X_train_cpy.NWP4_CLCT)\n",
    "    \n",
    "    X_test_cpy['U'] = (X_test_cpy.NWP1_U + X_test_cpy.NWP2_U + X_test_cpy.NWP3_U)/3\n",
    "    X_test_cpy['V'] = (X_test_cpy.NWP1_V + X_test_cpy.NWP2_V + X_test_cpy.NWP3_V)/3\n",
    "    X_test_cpy['T'] = (X_test_cpy.NWP1_T + X_test_cpy.NWP3_T)/2\n",
    "    X_test_cpy['CLCT'] = (X_test_cpy.NWP4_CLCT)\n",
    "\n",
    "    \n",
    "    X_train_cpy = X_train_cpy[['ID','Time','U','V','T','CLCT']]\n",
    "    X_test_cpy = X_test_cpy[['ID','Time','U','V','T','CLCT']]\n",
    "    \n",
    "    # Hay 11 valores perdidos en la columna CLCT en X_test_cpy. Los imputamos\n",
    "    X_test_cpy['CLCT'].fillna(method='bfill', limit=11, inplace=True)\n",
    "    \n",
    "     ####### Limpiar outliers y valores anómalos #######\n",
    "    \n",
    "    # valores negativos en CLCT\n",
    "    X_train_cpy.loc[X_train_cpy['CLCT'] < 0, 'CLCT'] = 0.0\n",
    "    X_test_cpy.loc[X_test_cpy['CLCT'] < 0, 'CLCT'] = 0.0\n",
    "\n",
    "    # añadir columna Production\n",
    "    X_train_cpy['Production'] = Y_train_cpy.to_list()\n",
    "    \n",
    "    # calcular módulo velocidad del viento\n",
    "    X_train_cpy['vel'] = X_train_cpy.apply(get_wind_velmod, axis=1)\n",
    "    \n",
    "    # formar matriz de datos \n",
    "    mm = PowerTransformer()\n",
    "\n",
    "    X1 = mm.fit_transform(np.array(X_train_cpy['vel']).reshape(-1,1))\n",
    "    X2 = mm.fit_transform(np.array(X_train_cpy['Production']).reshape(-1,1))\n",
    "    X = np.concatenate((X1,X2), axis=1)\n",
    "    \n",
    "    # algoritmo para detección de outliers\n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=20).fit(X)\n",
    "    threshold = pd.Series(clusterer.outlier_scores_).quantile(0.96)\n",
    "    outliers = np.where(clusterer.outlier_scores_ > threshold)[0]\n",
    "    \n",
    "    # Eliminamos los registros outliers \n",
    "    X_train_cpy.drop(X_train_cpy.index[list(outliers)], inplace=True)\n",
    "    \n",
    "    # Eliminamos las observaciones corresp#ondientes de Y_train\n",
    "    Y_train_cpy = Y_train_cpy.loc[X_train_cpy['ID'].values - 1]\n",
    "    \n",
    "    # Eliminamos las columnas 'vel' y 'Production'\n",
    "    del X_train_cpy['vel']\n",
    "    del X_train_cpy['Production']\n",
    "    \n",
    "    ###################################\n",
    "\n",
    "    # Pre-procesado de los datos\n",
    "    pre_process = ColumnTransformer(remainder = 'passthrough',\n",
    "                                    transformers = [('drop_columns', 'drop', ['ID','Time','U','V','T',\n",
    "                                                                             'month','hour','w_dir'])]\n",
    "    )\n",
    "\n",
    "    # definir pipeline\n",
    "    prepare_data_pipeline = Pipeline(steps=[\n",
    "        ('attr_adder', dtr.NewFeaturesAdder()), \n",
    "        ('pre_processing', pre_process)\n",
    "    ])\n",
    "\n",
    "\n",
    "    # aplicar pipeline\n",
    "    X_train_pped = prepare_data_pipeline.fit_transform(X_train_cpy, Y_train_cpy)\n",
    "    # print('Nº features antes de la seleccion: ', X_train_pped.shape[1])\n",
    "    \n",
    "    # seleccionar features más importantes mediante Random Forest\n",
    "    #rf = RandomForestRegressor(random_state=42)\n",
    "    #rf_reg = rf.fit(X_train_pped, Y_train_cpy)\n",
    "    #sel = SelectFromModel(rf_reg, prefit=True, threshold='2*median')\n",
    "    #X_train_pped = sel.transform(X_train_pped)\n",
    "    \n",
    "    # print('Nº features después de la seleccion: ', X_train_pped.shape[1])               \n",
    "\n",
    "\n",
    "    # creamos nuestro scorer basado en el CAPE\n",
    "    cape_scorer = make_scorer(metric.get_cape, greater_is_better=False)\n",
    "\n",
    "    # Modelización: Random Forest\n",
    "    # Entrenamiento del modelo mediante k-fold cross validation\n",
    "    # Búsqueda de hiperparámetros mediante Gridsearch \n",
    "    \n",
    "    param_grid = {   \n",
    "        'min_child_weight': [1, 5, 10],\n",
    "        'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'max_depth': [3, 4, 5],\n",
    "    }\n",
    "    \n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    xgb_reg = xgb.XGBRegressor(objective ='reg:squarederror')\n",
    "    grid_search_xgb = GridSearchCV(\n",
    "        xgb_reg, \n",
    "        param_grid, \n",
    "        cv=tscv,\n",
    "        scoring=cape_scorer,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    grid_search_xgb.fit(X_train_pped, Y_train_cpy)\n",
    "    \n",
    "    reg = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = grid_search_xgb.best_params_['colsample_bytree'],\n",
    "                           gamma = grid_search_xgb.best_params_['gamma'],\n",
    "                           max_depth = grid_search_xgb.best_params_['max_depth'],\n",
    "                           min_child_weight = grid_search_xgb.best_params_['min_child_weight'],\n",
    "                           subsample = grid_search_xgb.best_params_['subsample'],\n",
    "                           random_state=42)\n",
    "                                       \n",
    "    reg.fit(X_train_pped, Y_train_cpy)\n",
    "    \n",
    "    # evaluación sobre el conjunto de test\n",
    "    X_test_pped = prepare_data_pipeline.transform(X_test_cpy)\n",
    "    # X_test_pped = sel.transform(X_test_pped)\n",
    "    predictions = reg.predict(X_test_pped)\n",
    "    \n",
    "    # generamos matriz de predicciones (ID,Production)\n",
    "    pred_matrix = np.stack((np.array(ID_test).astype(int), predictions), axis=-1)\n",
    "    df_pred = pd.DataFrame(data=pred_matrix, columns=['ID','Production'])\n",
    "\n",
    "    # añadimos las predicciones al dataframe final que contendrá las de todas las WF\n",
    "    submission_df = submission_df.append(df_pred, ignore_index=True) \n",
    "    print('R2 for {} is {}'.format(WF, reg.score(X_train_pped, Y_train_cpy)))\n",
    "    print('Predictions for {} has been added to submission_df'.format(WF))\n",
    "    \n",
    "    scores_list.append(reg.score(X_train_pped, Y_train_cpy))\n",
    "\n",
    "    title = \"Learning Curves\"\n",
    "    cv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "    estimator = reg\n",
    "    plot_learning_curve(estimator, title, X_train_pped, Y_train_cpy, cv=cv, n_jobs=-1)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "global_score = np.mean(scores_list)\n",
    "\n",
    "print('********************************************')\n",
    "print('Global score: ', global_score)\n",
    "print('********************************************')\n",
    "\n",
    "\n",
    "# generamos fichero csv para el submission\n",
    "submission_df.to_csv(\"../../TFM/models/submission_xgb2.csv\", index=False, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_cpy.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba 5: Random Forest con validación Randomized Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "# Automatización de pre-procesado, modelización, validación y prediccion\n",
    "from sklearn.compose import ColumnTransformer \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from src.functions import metric\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# Lista de WFs\n",
    "WF_lst = X_train['WF'].unique()\n",
    "\n",
    "# lista para guardar los modelos, uno por WF\n",
    "models = []\n",
    "\n",
    "# Data frame para guardar las predicciones (ID, Production)\n",
    "submission_df = pd.DataFrame([], columns=['ID','Production']) \n",
    "\n",
    "for WF in WF_lst:\n",
    "    # Creamos copia de los datos para no perder su formato inicial\n",
    "    X_train_cpy = X_train.copy()\n",
    "    Y_train_cpy = Y_train.copy()\n",
    "    X_test_cpy = X_test.copy()\n",
    "\n",
    "    # Selección de filas por WF\n",
    "    X_train_cpy = X_train_cpy[X_train_cpy['WF'] == WF]\n",
    "    X_test_cpy = X_test_cpy[X_test_cpy['WF'] == WF]\n",
    "\n",
    "    # Identificador de observaciones\n",
    "    ID_train = X_train_cpy['ID']\n",
    "    ID_test = X_test_cpy['ID']\n",
    "\n",
    "    # Seleccion de las filas de Y_train\n",
    "    Y_train_cpy = Y_train_cpy['Production']\n",
    "    Y_train_cpy = Y_train_cpy.loc[ID_train.values - 1]\n",
    "\n",
    "    # Pre-procesado de los datos en bruto\n",
    "\n",
    "    # We'll add new columns NWPX_<met_var> without missing values\n",
    "    # new cols: NWP1_U, NWP1_V, NWP1_T, NWP2_U, NWP2_V, NWP3_U, NWP3_V, NWP3_T, NWP4_U, NWP4_V, NWP4_CLCT\n",
    "    \n",
    "    new_cols = ['NWP1_U','NWP1_V','NWP1_T','NWP2_U',\n",
    "                'NWP2_V','NWP3_U','NWP3_V','NWP3_T',\n",
    "                'NWP4_U','NWP4_V','NWP4_CLCT']\n",
    "\n",
    "    def add_new_cols(new_cols, df):\n",
    "        for col in new_cols:\n",
    "            df[col] = np.nan    \n",
    "            \n",
    "    add_new_cols(new_cols, X_train_cpy)\n",
    "    add_new_cols(new_cols, X_test_cpy)\n",
    "    \n",
    "    cols_train = X_train.columns[3:]\n",
    "    cols_test = X_test.columns[3:-9]\n",
    "    X_train_cpy = input_missing_values(X_train_cpy, cols_train)\n",
    "    X_test_cpy = input_missing_values(X_test_cpy, cols_test)\n",
    "    \n",
    "    col_list = ['NWP2_U','NWP2_V','NWP3_U','NWP3_V','NWP3_T']\n",
    "    X_train_cpy.index = X_train_cpy['Time']\n",
    "    \n",
    "    del X_train_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_train_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=2,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "\n",
    "    \n",
    "    X_train_cpy.reset_index(inplace=True)\n",
    "\n",
    "    X_test_cpy.index = X_test_cpy['Time']\n",
    "    \n",
    "    del X_test_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_test_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=100,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "    \n",
    "        \n",
    "    X_test_cpy.reset_index(inplace=True) \n",
    "\n",
    "    X_train_cpy['U'] = (X_train_cpy.NWP1_U + X_train_cpy.NWP2_U + X_train_cpy.NWP3_U + X_train_cpy.NWP4_U)/4\n",
    "    X_train_cpy['V'] = (X_train_cpy.NWP1_V + X_train_cpy.NWP2_V + X_train_cpy.NWP3_V + X_train_cpy.NWP4_V)/4\n",
    "    X_train_cpy['T'] = (X_train_cpy.NWP1_T + X_train_cpy.NWP3_T)/2\n",
    "    X_train_cpy['CLCT'] = (X_train_cpy.NWP4_CLCT)\n",
    "    X_train_cpy['T2'] = np.sqrt(X_train_cpy['T'])\n",
    "    \n",
    "    X_test_cpy['U'] = (X_test_cpy.NWP1_U + X_test_cpy.NWP2_U + X_test_cpy.NWP3_U)/3\n",
    "    X_test_cpy['V'] = (X_test_cpy.NWP1_V + X_test_cpy.NWP2_V + X_test_cpy.NWP3_V)/3\n",
    "    X_test_cpy['T'] = (X_test_cpy.NWP1_T + X_test_cpy.NWP3_T)/2\n",
    "    X_test_cpy['CLCT'] = (X_test_cpy.NWP4_CLCT)\n",
    "    X_test_cpy['T2'] = np.sqrt(X_test_cpy['T'])\n",
    "\n",
    "    \n",
    "    X_train_cpy = X_train_cpy[['ID','Time','U','V','T','CLCT','T2']]\n",
    "    X_test_cpy = X_test_cpy[['ID','Time','U','V','T','CLCT','T2']]\n",
    "    \n",
    "    # Hay 11 valores perdidos en la columna CLCT en X_test_cpy. Los imputamos con la moda\n",
    "    X_test_cpy.fillna(method='bfill', limit=11, inplace=True)\n",
    "    \n",
    "    ####### Limpiar outliers #######\n",
    "    \n",
    "    # añadir columna Production\n",
    "    X_train_cpy['Production'] = Y_train_cpy.to_list()\n",
    "    \n",
    "    # calcular módulo velocidad del viento\n",
    "    X_train_cpy['vel'] = X_train_cpy.apply(get_wind_velmod, axis=1)\n",
    "    \n",
    "    # formar matriz de datos \n",
    "    X1 = X_train_cpy['vel'].values.reshape(-1,1)\n",
    "    X2 = X_train_cpy['Production'].values.reshape(-1,1)\n",
    "    X = np.concatenate((X1,X2), axis=1)\n",
    "    \n",
    "    # Definir el clasificador de outliers\n",
    "    clf = OneClassSVM(nu=0.17, gamma=0.06)\n",
    "    clf.fit(X)\n",
    "    \n",
    "    # Predección de outlier o inlier para cada punto\n",
    "    y_pred = clf.predict(X)\n",
    "    \n",
    "    # Añadirmos la columna 'oulier' con la predicción \n",
    "    X_train_cpy['outlier'] = y_pred.tolist()\n",
    "    \n",
    "    # Eliminamos los registros outliers \n",
    "    X_train_cpy = X_train_cpy[X_train_cpy['outlier'] != -1]\n",
    "    \n",
    "    # Eliminamos las observaciones correspondientes de Y_train\n",
    "    Y_train_cpy = Y_train_cpy.loc[X_train_cpy['ID'].values - 1]\n",
    "    \n",
    "    # Eliminamos las columnas 'vel', 'outlier' y 'Production'\n",
    "    del X_train_cpy['vel']\n",
    "    del X_train_cpy['outlier']\n",
    "    del X_train_cpy['Production']\n",
    "    \n",
    "    ###################################\n",
    "\n",
    "    # Pre-procesado de los datos\n",
    "    pre_process = ColumnTransformer(remainder = 'passthrough',\n",
    "                                    transformers = [('drop_columns', 'drop', \n",
    "                                                     ['ID','Time','U','V','T2'])]\n",
    "    )\n",
    "\n",
    "    # definir pipeline\n",
    "    prepare_data_pipeline = Pipeline(steps=[\n",
    "        ('attr_adder', dtr.NewFeaturesAdder(add_w_shear=False, add_time_feat=False)), \n",
    "        ('pre_processing', pre_process),\n",
    "        ('std_scaler', StandardScaler())\n",
    "    ])\n",
    "    # aplciar pipeline\n",
    "    X_train_pped = prepare_data_pipeline.fit_transform(X_train_cpy)\n",
    "\n",
    "    # creamos nuestro scorer basado en el CAPE\n",
    "    cape_scorer = make_scorer(metric.get_cape, greater_is_better=False)\n",
    "\n",
    "    # Modelización: Random Forest\n",
    "    # Entrenamiento del modelo mediante k-fold cross validation\n",
    "    # Búsqueda de hiperparámetros mediante Randomized Search \n",
    "\n",
    "    forest_reg = RandomForestRegressor()\n",
    "\n",
    "    rf_random = RandomizedSearchCV(\n",
    "        estimator = forest_reg, \n",
    "        param_distributions = random_grid, \n",
    "        n_iter = 100, \n",
    "        cv = 5, \n",
    "        random_state=42, \n",
    "        scoring = cape_scorer,\n",
    "        n_jobs = -1\n",
    "    )\n",
    "\n",
    "    rf_random.fit(X_train_pped, Y_train_cpy)\n",
    "    final_model = rf_random.best_estimator_\n",
    "\n",
    "    # guardamos modelo\n",
    "    models.append(joblib.dump(final_model, WF + 'rfrand'))\n",
    "\n",
    "    # evaluación sobre el conjunto de test\n",
    "    X_test_pped = prepare_data_pipeline.transform(X_test_cpy)\n",
    "    predictions = final_model.predict(X_test_pped)\n",
    "\n",
    "    # generamos matriz de predicciones (ID,Production)\n",
    "    pred_matrix = np.stack((np.array(ID_test).astype(int), predictions), axis=-1)\n",
    "    df_pred = pd.DataFrame(data=pred_matrix, columns=['ID','Production'])\n",
    "\n",
    "    # añadimos las predicciones al dataframe final que contendrá las de todas las WF\n",
    "    submission_df = submission_df.append(df_pred, ignore_index=True)  \n",
    "    print('Best score for {}: {}'.format(WF, -rf_random.best_score_))\n",
    "    print('Predictions for {} has been added to submission_df'.format(WF))\n",
    "    print('--------')\n",
    "\n",
    "\n",
    "# generamos fichero csv para el submission\n",
    "submission_df.to_csv(\"./Data/submission_rfrand.csv\", index=False, sep=\",\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba 6: Regresión con Elastic Net (Ridge + Lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Automatización de pre-procesado, modelización, validación y prediccion\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor \n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PowerTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from src.functions import metric\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.externals import joblib\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "# Lista de WFs\n",
    "WF_lst = X_train['WF'].unique()\n",
    "\n",
    "# lista para guardar los modelos, uno por WF\n",
    "# models = []\n",
    "\n",
    "# Data frame para guardar las predicciones (ID, Production)\n",
    "submission_df = pd.DataFrame([], columns=['ID','Production']) \n",
    "\n",
    "for WF in WF_lst:\n",
    "    # Creamos copia de los datos para no perder su formato inicial\n",
    "    X_train_cpy = X_train.copy()\n",
    "    Y_train_cpy = Y_train.copy()\n",
    "    X_test_cpy = X_test.copy()\n",
    "\n",
    "    # Selección de filas por WF\n",
    "    X_train_cpy = X_train_cpy[X_train_cpy['WF'] == WF]\n",
    "    X_test_cpy = X_test_cpy[X_test_cpy['WF'] == WF]\n",
    "\n",
    "    # Identificador de observaciones\n",
    "    ID_train = X_train_cpy['ID']\n",
    "    ID_test = X_test_cpy['ID']\n",
    "\n",
    "    # Seleccion de las filas de Y_train\n",
    "    Y_train_cpy = Y_train_cpy['Production']\n",
    "    Y_train_cpy = Y_train_cpy.loc[ID_train.values - 1]\n",
    "\n",
    "    # Pre-procesado de los datos en bruto\n",
    "\n",
    "    # We'll add new columns NWPX_<met_var> without missing values\n",
    "    # new cols: NWP1_U, NWP1_V, NWP1_T, NWP2_U, NWP2_V, NWP3_U, NWP3_V, NWP3_T, NWP4_U, NWP4_V, NWP4_CLCT\n",
    "    \n",
    "    new_cols = ['NWP1_U','NWP1_V','NWP1_T','NWP2_U',\n",
    "                'NWP2_V','NWP3_U','NWP3_V','NWP3_T',\n",
    "                'NWP4_U','NWP4_V','NWP4_CLCT']\n",
    "\n",
    "    def add_new_cols(new_cols, df):\n",
    "        for col in new_cols:\n",
    "            df[col] = np.nan    \n",
    "            \n",
    "    add_new_cols(new_cols, X_train_cpy)\n",
    "    add_new_cols(new_cols, X_test_cpy)\n",
    "    \n",
    "    cols_train = X_train.columns[3:]\n",
    "    cols_test = X_test.columns[3:-9]\n",
    "    X_train_cpy = input_missing_values(X_train_cpy, cols_train)\n",
    "    X_test_cpy = input_missing_values(X_test_cpy, cols_test)\n",
    "    \n",
    "    col_list = ['NWP2_U','NWP2_V','NWP3_U','NWP3_V','NWP3_T']\n",
    "    X_train_cpy.index = X_train_cpy['Time']\n",
    "    \n",
    "    del X_train_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_train_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=2,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "\n",
    "    \n",
    "    X_train_cpy.reset_index(inplace=True)\n",
    "\n",
    "    X_test_cpy.index = X_test_cpy['Time']\n",
    "    \n",
    "    del X_test_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_test_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=100,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "    \n",
    "        \n",
    "    X_test_cpy.reset_index(inplace=True) \n",
    "\n",
    "    X_train_cpy['U'] = X_train_cpy.NWP1_U\n",
    "    X_train_cpy['V'] = X_train_cpy.NWP1_V\n",
    "    X_train_cpy['T'] = X_train_cpy.NWP3_T\n",
    "    X_train_cpy['CLCT'] = X_train_cpy.NWP4_CLCT\n",
    "    \n",
    "    X_test_cpy['U'] = X_test_cpy.NWP1_U\n",
    "    X_test_cpy['V'] = X_test_cpy.NWP1_V\n",
    "    X_test_cpy['T'] = X_test_cpy.NWP3_T\n",
    "    X_test_cpy['CLCT'] = X_test_cpy.NWP4_CLCT\n",
    " \n",
    "    X_train_cpy = X_train_cpy[['ID','Time','U','V','T','CLCT']]\n",
    "    X_test_cpy = X_test_cpy[['ID','Time','U','V','T','CLCT']]\n",
    "    \n",
    "    # Hay 11 valores perdidos en la columna CLCT en X_test_cpy. Los imputamos\n",
    "    X_test_cpy.fillna(method='bfill', limit=11, inplace=True)\n",
    "\n",
    "    ####### Limpiar outliers #######\n",
    "    # Negative values in CLCT\n",
    "    X_train_cpy.loc[X_train_cpy['CLCT'] < 0, 'CLCT'] = 0.0\n",
    "    X_test_cpy.loc[X_test_cpy['CLCT'] < 0, 'CLCT'] = 0.0\n",
    "    \n",
    "    # Locating and removing outliers based on wind power curve\n",
    "    # Add 'Production' column\n",
    "    X_train_cpy['Production'] = Y_train_cpy.to_list()\n",
    "    \n",
    "    # Calculate wind velocity module\n",
    "    X_train_cpy['vel'] = X_train_cpy.apply(dtr.get_wind_velmod, axis=1)\n",
    "    \n",
    "    # Build data matrix\n",
    "    mm = PowerTransformer(method='yeo-johnson', standardize=True)\n",
    "    \n",
    "    X1 = mm.fit_transform(X_train_cpy['vel'].values.reshape(-1,1))\n",
    "    X2 = mm.fit_transform(X_train_cpy['Production'].values.reshape(-1,1))\n",
    "    X = np.concatenate((X1,X2), axis=1)\n",
    "    \n",
    "    # Using DBSCAN to find outliers\n",
    "    from sklearn.cluster import DBSCAN\n",
    "    outlier_detection = DBSCAN(\n",
    "        eps = 0.1,\n",
    "        metric=\"l1\",\n",
    "        min_samples = 10,\n",
    "        n_jobs = -1)\n",
    "    \n",
    "    clusters = outlier_detection.fit_predict(X)\n",
    "    outliers = np.where(clusters == -1)[0]\n",
    "\n",
    "    # Deleting columns 'vel' and 'Production'\n",
    "    del X_train_cpy['vel']\n",
    "    del X_train_cpy['Production']\n",
    "    ###################################\n",
    "\n",
    "    pre_process = ColumnTransformer(remainder = 'passthrough',\n",
    "                                    transformers = [('drop_columns', 'drop', \n",
    "                                                     ['ID','Time','U','V',\n",
    "                                                      'month_sin','month_cos',\n",
    "                                                      'w_dir','month','hour'\n",
    "                                                      ])]\n",
    "    )\n",
    "    \n",
    "    prepare_data_pipeline = Pipeline(steps=[\n",
    "        ('attr_adder', dtr.NewFeaturesAdder(add_cycl_feat=True, add_time_feat=True)), \n",
    "        ('pre_processing', pre_process),\n",
    "        ('power_transf', PowerTransformer())\n",
    "    ])\n",
    "\n",
    "    # aplicar pipeline\n",
    "    X_train_pped = prepare_data_pipeline.fit_transform(X_train_cpy)\n",
    "    \n",
    "    # Deleting outlier related observations\n",
    "    X_train_pped = np.delete(X_train_pped, tuple(outliers), axis=0)\n",
    "\n",
    "    # Deleting the corresponding observations in y_train\n",
    "    Y_train_cpy = np.delete(Y_train_cpy.to_numpy(), list(outliers))\n",
    "    \n",
    "    # creamos nuestro scorer basado en el CAPE\n",
    "    cape_scorer = make_scorer(metric.get_cape, greater_is_better=False)\n",
    "\n",
    "    # Modelización: Regresión polinomial con regularización Elastic Net\n",
    "    # Entrenamiento del modelo mediante k-fold cross validation\n",
    "    # Búsqueda de hiperparámetros mediante Gridsearch \n",
    "    \n",
    "    poly_features = PolynomialFeatures(degree=3, include_bias=False)\n",
    "    X_train_poly = poly_features.fit_transform(X_train_pped)\n",
    "    \n",
    "    param_grid = [{\n",
    "        'alpha'     : np.logspace(-3, -2, 1, 2, 3),\n",
    "        'l1_ratio'  : [0.00, 0.25, 0.50, 0.75, 1.0],\n",
    "        'tol'       : [0.00001, 0.0001, 0.001]\n",
    "    }]\n",
    "    \n",
    "                                               \n",
    "    eNet = ElasticNet(selection='random')\n",
    "    grid_search = GridSearchCV(\n",
    "        eNet, \n",
    "        param_grid, \n",
    "        cv = TimeSeriesSplit(n_splits=7),\n",
    "        scoring=cape_scorer,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    grid_search.fit(X_train_poly, Y_train_cpy)\n",
    "    \n",
    "    # Calculate cross validation scores\n",
    "    cvres = grid_search.cv_results_\n",
    "    print('Best CV score:', -grid_search.best_score_)\n",
    "\n",
    "    # Re-training without CV, using the best param#eters obtained by CV\n",
    "    eNet2 = ElasticNet(selection='random',\n",
    "                       alpha=grid_search.best_params_['alpha'],\n",
    "                       l1_ratio=grid_search.best_params_['l1_ratio'],\n",
    "                       tol=grid_search.best_params_['tol']\n",
    "                      )\n",
    "    \n",
    "    \n",
    "    ttreg = TransformedTargetRegressor(regressor=eNet2, \n",
    "                                       transformer=PowerTransformer(), \n",
    "                                       check_inverse=False)\n",
    "    \n",
    "    ttreg.fit(X_train_poly, Y_train_cpy)\n",
    "            \n",
    "    # evaluación sobre el conjunto de test\n",
    "    X_test_pped = prepare_data_pipeline.transform(X_test_cpy)\n",
    "    X_test_poly = poly_features.fit_transform(X_test_pped)\n",
    "    predictions = ttreg.predict(X_test_poly)\n",
    "    \n",
    "    # build prediction matrix (ID,Production)\n",
    "    pred_matrix = np.stack((np.array(ID_test).astype(int), predictions), axis=-1)\n",
    "    df_pred = pd.DataFrame(data=pred_matrix, columns=['ID','Production'])\n",
    "    \n",
    "    # añadimos las predicciones al dataframe final que contendrá las de todas las WF\n",
    "    submission_df = submission_df.append(df_pred, ignore_index=True) \n",
    "    # print('R^2 for {} is {}'.format(WF, ttreg.score(X_train_poly, Y_train_cpy)))\n",
    "    print('Predictions for {} has been added to submission_df'.format(WF))\n",
    "    print('---------------')\n",
    "\n",
    "# generamos fichero csv para el submission\n",
    "submission_df.to_csv(\"../../TFM/models/elasticNet_2.csv\", index=False, sep=\",\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pped.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba 7: LassoCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Automatización de pre-procesado, modelización, validación y prediccion\n",
    "from sklearn.compose import ColumnTransformer \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from src.functions import metric\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import linear_model as lm\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "# Lista de WFs\n",
    "WF_lst = X_train['WF'].unique()\n",
    "\n",
    "# lista para guardar los modelos, uno por WF\n",
    "models = []\n",
    "\n",
    "# Data frame para guardar las predicciones (ID, Production)\n",
    "submission_df = pd.DataFrame([], columns=['ID','Production']) \n",
    "\n",
    "\n",
    "\n",
    "for WF in WF_lst:\n",
    "    # Creamos copia de los datos para no perder su formato inicial\n",
    "    X_train_cpy = X_train.copy()\n",
    "    Y_train_cpy = Y_train.copy()\n",
    "    X_test_cpy = X_test.copy()\n",
    "\n",
    "    # Selección de filas por WF\n",
    "    X_train_cpy = X_train_cpy[X_train_cpy['WF'] == WF]\n",
    "    X_test_cpy = X_test_cpy[X_test_cpy['WF'] == WF]\n",
    "\n",
    "    # Identificador de observaciones\n",
    "    ID_train = X_train_cpy['ID']\n",
    "    ID_test = X_test_cpy['ID']\n",
    "\n",
    "    # Seleccion de las filas de Y_train\n",
    "    Y_train_cpy = Y_train_cpy['Production']\n",
    "    Y_train_cpy = Y_train_cpy.loc[ID_train.values - 1]\n",
    "\n",
    "    # Pre-procesado de los datos en bruto\n",
    "\n",
    "    # We'll add new columns NWPX_<met_var> without missing values\n",
    "    # new cols: NWP1_U, NWP1_V, NWP1_T, NWP2_U, NWP2_V, NWP3_U, NWP3_V, NWP3_T, NWP4_U, NWP4_V, NWP4_CLCT\n",
    "    \n",
    "    new_cols = ['NWP1_U','NWP1_V','NWP1_T','NWP2_U',\n",
    "                'NWP2_V','NWP3_U','NWP3_V','NWP3_T',\n",
    "                'NWP4_U','NWP4_V','NWP4_CLCT']\n",
    "\n",
    "    def add_new_cols(new_cols, df):\n",
    "        for col in new_cols:\n",
    "            df[col] = np.nan    \n",
    "            \n",
    "    add_new_cols(new_cols, X_train_cpy)\n",
    "    add_new_cols(new_cols, X_test_cpy)\n",
    "    \n",
    "    cols_train = X_train_cpy.columns[3:-11]\n",
    "    cols_test = X_test_cpy.columns[3:-11]\n",
    "    X_train_cpy = input_missing_values(X_train_cpy, cols_train)\n",
    "    X_test_cpy = input_missing_values(X_test_cpy, cols_train)\n",
    "    \n",
    "    col_list = ['NWP2_U','NWP2_V','NWP3_U','NWP3_V','NWP3_T']\n",
    "    X_train_cpy.index = X_train_cpy['Time']\n",
    "    \n",
    "    del X_train_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_train_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=2,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "\n",
    "    \n",
    "    X_train_cpy.reset_index(inplace=True)\n",
    "\n",
    "    X_test_cpy.index = X_test_cpy['Time']\n",
    "    \n",
    "    del X_test_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_test_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=100,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "    \n",
    "        \n",
    "    X_test_cpy.reset_index(inplace=True) \n",
    "\n",
    "    X_train_cpy['U'] = (X_train_cpy.NWP1_U + X_train_cpy.NWP2_U + X_train_cpy.NWP3_U + X_train_cpy.NWP4_U)/4\n",
    "    X_train_cpy['V'] = (X_train_cpy.NWP1_V + X_train_cpy.NWP2_V + X_train_cpy.NWP3_V + X_train_cpy.NWP4_V)/4\n",
    "    X_train_cpy['T'] = (X_train_cpy.NWP1_T + X_train_cpy.NWP3_T)/2\n",
    "    X_train_cpy['CLCT'] = (X_train_cpy.NWP4_CLCT)\n",
    "    X_train_cpy['inv_T'] = 1/(X_train_cpy['T'])\n",
    "    \n",
    "    X_test_cpy['U'] = (X_test_cpy.NWP1_U + X_test_cpy.NWP2_U + X_test_cpy.NWP3_U)/3\n",
    "    X_test_cpy['V'] = (X_test_cpy.NWP1_V + X_test_cpy.NWP2_V + X_test_cpy.NWP3_V)/3\n",
    "    X_test_cpy['T'] = (X_test_cpy.NWP1_T + X_test_cpy.NWP3_T)/2\n",
    "    X_test_cpy['CLCT'] = (X_test_cpy.NWP4_CLCT)\n",
    "    X_test_cpy['inv_T'] = 1/(X_test_cpy['T'])\n",
    "    \n",
    "    X_train_cpy = X_train_cpy[['Time','U','V','T','CLCT','inv_T']]\n",
    "    X_test_cpy = X_test_cpy[['Time','U','V','T','CLCT', 'inv_T']]\n",
    "\n",
    "    \n",
    "    ## Limpieza de datos: imputar valores perdidos en X_\n",
    "    #only_na = X_test_cpy[~X_test_cpy.ID.isin(X_test_cpy.dropna().ID)]\n",
    "    #X_test_cpy.dropna(inplace=True)\n",
    "    #Y_train_cpy.drop(labels=only_na.index, axis=0, inplace=True)\n",
    "    #X_train_cpy.drop(labels=only_na.index, axis=0, inplace=True)\n",
    "\n",
    "    # Pre-procesado de los datos\n",
    "    pre_process = ColumnTransformer(remainder = 'passthrough',\n",
    "                                    transformers = [('drop_columns', 'drop', \n",
    "                                                     ['Time','U','V','T'])]\n",
    "    )\n",
    "\n",
    "    # definir pipeline\n",
    "    prepare_data_pipeline = Pipeline(steps=[\n",
    "        ('attr_adder', dtr.DerivedAttributesAdder(add_time_feat=False, add_cyclic_feat=False)), \n",
    "        ('pre_processing', pre_process),\n",
    "        ('std_scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    # aplicar pipeline\n",
    "    X_train_pped = prepare_data_pipeline.fit_transform(X_train_cpy)\n",
    "\n",
    "\n",
    "    # creamos nuestro scorer basado en el CAPE\n",
    "    cape_scorer = make_scorer(metric.get_cape, greater_is_better=False)\n",
    "\n",
    "    # Modelización: Regresión polinomial utilizando CV con regularización tipo Ride\n",
    "    poly_features = PolynomialFeatures(degree=3, include_bias=False)\n",
    "    X_train_poly = poly_features.fit_transform(X_train_pped)\n",
    "    \n",
    "    param_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "    lasso_reg = lm.Lasso()\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        lasso_reg, \n",
    "        param_grid, \n",
    "        cv=7,\n",
    "        scoring=cape_scorer,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    grid_search.fit(X_train_poly, Y_train_cpy)\n",
    "    final_model = grid_search.best_estimator_\n",
    "    \n",
    "    # guardamos modelo\n",
    "    models.append(joblib.dump(final_model, WF + '_lasso'))\n",
    "\n",
    "    # evaluación sobre el conjunto de test\n",
    "    X_test_pped = prepare_data_pipeline.transform(X_test_cpy)\n",
    "    X_test_poly = poly_features.fit_transform(X_test_pped)\n",
    "    predictions = final_model.predict(X_test_poly)\n",
    "    \n",
    "    # generamos matriz de predicciones (ID,Production)\n",
    "    pred_matrix = np.stack((np.array(ID_test).astype(int), predictions), axis=-1)\n",
    "    df_pred = pd.DataFrame(data=pred_matrix, columns=['ID','Production'])\n",
    "\n",
    "    # añadimos las predicciones al dataframe final que contendrá las de todas las WF\n",
    "    submission_df = submission_df.append(df_pred, ignore_index=True) \n",
    "    print('CAPE for {} is {}'.format(WF, -grid_search.best_score_))\n",
    "    print('Predictions for {} has been added to submission_df'.format(WF))\n",
    "\n",
    "# generamos fichero csv para el submission\n",
    "submission_df.to_csv(\"../../TFM/models/submission_lasso.csv\", index=False, sep=\",\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba 8: Regresión robusta con RANSAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Automatización de pre-procesado, modelización, validación y prediccion\n",
    "from sklearn.compose import ColumnTransformer \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from src.functions import metric\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import linear_model as lm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import RANSACRegressor\n",
    "\n",
    "# Lista de WFs\n",
    "WF_lst = X_train['WF'].unique()\n",
    "\n",
    "# lista para guardar los modelos, uno por WF\n",
    "models = []\n",
    "\n",
    "# Data frame para guardar las predicciones (ID, Production)\n",
    "submission_df = pd.DataFrame([], columns=['ID','Production']) \n",
    "\n",
    "for WF in WF_lst:\n",
    "    # Creamos copia de los datos para no perder su formato inicial\n",
    "    X_train_cpy = X_train.copy()\n",
    "    Y_train_cpy = Y_train.copy()\n",
    "    X_test_cpy = X_test.copy()\n",
    "\n",
    "    # Selección de filas por WF\n",
    "    X_train_cpy = X_train_cpy[X_train_cpy['WF'] == WF]\n",
    "    X_test_cpy = X_test_cpy[X_test_cpy['WF'] == WF]\n",
    "\n",
    "    # Identificador de observaciones\n",
    "    ID_train = X_train_cpy['ID']\n",
    "    ID_test = X_test_cpy['ID']\n",
    "\n",
    "    # Seleccion de las filas de Y_train\n",
    "    Y_train_cpy = Y_train_cpy['Production']\n",
    "    Y_train_cpy = Y_train_cpy.loc[ID_train.values - 1]\n",
    "\n",
    "    # Pre-procesado de los datos en bruto\n",
    "    X_train_cpy = X_train_cpy[['Time','NWP1_00h_D-1_U','NWP1_00h_D-1_V','NWP1_00h_D-1_T']]\n",
    "    X_train_cpy.rename(\n",
    "        columns={\"Time\": \"time\", \"NWP1_00h_D-1_U\": \"U\", \"NWP1_00h_D-1_V\": \"V\", \"NWP1_00h_D-1_T\":\"T\"}, \n",
    "        inplace=True)\n",
    "\n",
    "    X_test_cpy = X_test_cpy[['Time','NWP1_00h_D-1_U','NWP1_00h_D-1_V','NWP1_00h_D-1_T']]\n",
    "    X_test_cpy.rename(\n",
    "        columns={\"Time\": \"time\", \"NWP1_00h_D-1_U\": \"U\", \"NWP1_00h_D-1_V\": \"V\", \"NWP1_00h_D-1_T\":\"T\"}, \n",
    "        inplace=True)\n",
    "\n",
    "    # Limpieza de datos: eliminar valores perdidos\n",
    "    only_na = X_train_cpy[~X_train_cpy.index.isin(X_train_cpy.dropna().index)]\n",
    "    X_train_cpy.dropna(inplace=True)\n",
    "    Y_train_cpy.drop(labels=only_na.index, axis=0, inplace=True)\n",
    "\n",
    "    X_test_cpy.dropna(inplace=True)\n",
    "\n",
    "    # Pre-procesado de los datos\n",
    "    pre_process = ColumnTransformer(remainder = 'passthrough',\n",
    "                                    transformers = [('drop_columns', 'drop', ['time','U','V','T','w_dir'])]\n",
    "    )\n",
    "\n",
    "    # definir pipeline\n",
    "    prepare_data_pipeline = Pipeline(steps=[\n",
    "        ('attr_adder', dtr.DerivedAttributesAdder(add_time_feat=False)), \n",
    "        ('pre_processing', pre_process),\n",
    "        ('std_scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    # aplicar pipeline\n",
    "    X_train_pped = prepare_data_pipeline.fit_transform(X_train_cpy)\n",
    "\n",
    "    # creamos nuestro scorer basado en el CAPE\n",
    "    cape_scorer = make_scorer(metric.get_cape, greater_is_better=False)\n",
    "\n",
    "    # Modelización: Regresión Robusta utilizando RANSAC\n",
    "    poly_features = PolynomialFeatures(degree=3, include_bias=False)\n",
    "    X_train_poly = poly_features.fit_transform(X_train_pped)\n",
    "    \n",
    "    ransac = RANSACRegressor(LinearRegression(min_sa), loss='absolute_loss')\n",
    "    param_grid = [\n",
    "        {\n",
    "            'max_trials': [100, 1000, 10000],\n",
    "            'min_samples': [10, 30, 50],\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    grid_search_ransac = GridSearchCV(\n",
    "        ransac, \n",
    "        param_grid, \n",
    "        cv=5,\n",
    "        scoring=cape_scorer\n",
    "    )\n",
    "\n",
    "    grid_search_ransac.fit(X_train_poly, Y_train_cpy)\n",
    "    final_model = grid_search_ransac.best_estimator_\n",
    "    \n",
    "    # guardamos modelo\n",
    "    models.append(joblib.dump(final_model, WF + '_ransac'))\n",
    "\n",
    "    # evaluación sobre el conjunto de test\n",
    "    X_test_pped = prepare_data_pipeline.transform(X_test_cpy)\n",
    "    X_test_poly = poly_features.fit_transform(X_test_pped)\n",
    "    predictions = final_model.predict(X_test_poly)\n",
    "    \n",
    "    # generamos matriz de predicciones (ID,Production)\n",
    "    pred_matrix = np.stack((np.array(ID_test).astype(int), predictions), axis=-1)\n",
    "    df_pred = pd.DataFrame(data=pred_matrix, columns=['ID','Production'])\n",
    "\n",
    "    # añadimos las predicciones al dataframe final que contendrá las de todas las WF\n",
    "    submission_df = submission_df.append(df_pred, ignore_index=True) \n",
    "    print('Predictions for {} has been added to submission_df'.format(WF))\n",
    "    print('CAPE for {} is {}'.format(WF, -grid_search_ransac.best_score_))\n",
    "\n",
    "# generamos fichero csv para el submission\n",
    "submission_df.to_csv(\"./Data/submission_ransac.csv\", index=False, sep=\",\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo 10: MARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Quark\\.conda\\envs\\TFM-env\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning:\n",
      "\n",
      "`should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-3041e7001ef1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBaseEstimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTransformerMixin\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscorer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmake_scorer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "# Automatización de pre-procesado, modelización, validación y prediccion\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PowerTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from src.functions import metric\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import linear_model as lm\n",
    "from pyearth import Earth\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "np.warnings.filterwarnings('ignore')\n",
    "import hdbscan\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_regression\n",
    "\n",
    "# Lista de WFs\n",
    "WF_lst = ['WF2']\n",
    "\n",
    "# lista para guardar los modelos, uno por WF\n",
    "models = []\n",
    "scores_list = []\n",
    "\n",
    "# Data frame para guardar las predicciones (ID, Production)\n",
    "submission_df = pd.DataFrame([], columns=['ID','Production']) \n",
    "\n",
    "for WF in WF_lst:\n",
    "    # Creamos copia de los datos para no perder su formato inicial\n",
    "    X_train_cpy = X_train.copy()\n",
    "    Y_train_cpy = Y_train.copy()\n",
    "    X_test_cpy = X_test.copy()\n",
    "\n",
    "    # Selección de filas por WF\n",
    "    X_train_cpy = X_train_cpy[X_train_cpy['WF'] == WF]\n",
    "    X_test_cpy = X_test_cpy[X_test_cpy['WF'] == WF]\n",
    "\n",
    "    # Identificador de observaciones\n",
    "    ID_train = X_train_cpy['ID']\n",
    "    ID_test = X_test_cpy['ID']\n",
    "\n",
    "    # Seleccion de las filas de Y_train\n",
    "    Y_train_cpy = Y_train_cpy.loc[ID_train.values - 1]\n",
    "\n",
    "    # Pre-procesado de los datos en bruto\n",
    "\n",
    "    # We'll add new columns NWPX_<met_var> without missing values\n",
    "    # new cols: NWP1_U, NWP1_V, NWP1_T, NWP2_U, NWP2_V, NWP3_U, NWP3_V, NWP3_T, NWP4_U, NWP4_V, NWP4_CLCT\n",
    "    \n",
    "    new_cols = ['NWP1_U','NWP1_V','NWP1_T','NWP2_U',\n",
    "                'NWP2_V','NWP3_U','NWP3_V','NWP3_T',\n",
    "                'NWP4_U','NWP4_V','NWP4_CLCT']\n",
    "\n",
    "    def add_new_cols(new_cols, df):\n",
    "        for col in new_cols:\n",
    "            df[col] = np.nan    \n",
    "            \n",
    "    add_new_cols(new_cols, X_train_cpy)\n",
    "    add_new_cols(new_cols, X_test_cpy)\n",
    "    \n",
    "    cols_train = X_train.columns[3:]\n",
    "    cols_test = X_test.columns[3:-9]\n",
    "    X_train_cpy = input_missing_values(X_train_cpy, cols_train)\n",
    "    X_test_cpy = input_missing_values(X_test_cpy, cols_train)\n",
    "    \n",
    "    col_list = ['NWP2_U','NWP2_V','NWP3_U','NWP3_V','NWP3_T','NWP4_CLCT']\n",
    "    X_train_cpy.index = X_train_cpy['Time']\n",
    "    \n",
    "    del X_train_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_train_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=100,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "        \n",
    "    X_train_cpy.reset_index(inplace=True)\n",
    "    \n",
    "    X_test_cpy.index = X_test_cpy['Time']\n",
    "    \n",
    "    del X_test_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_test_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=100,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "    \n",
    "      \n",
    "    X_test_cpy.reset_index(inplace=True) \n",
    "\n",
    "    X_train_cpy['U'] = (X_train_cpy.NWP1_U + X_train_cpy.NWP2_U + X_train_cpy.NWP3_U)/3\n",
    "    X_train_cpy['V'] = (X_train_cpy.NWP1_V + X_train_cpy.NWP2_V + X_train_cpy.NWP3_V )/3\n",
    "    X_train_cpy['T'] = (X_train_cpy.NWP1_T + X_train_cpy.NWP3_T)/2\n",
    "    X_train_cpy['CLCT'] = X_train_cpy.NWP4_CLCT\n",
    "    \n",
    "    X_test_cpy['U'] = (X_test_cpy.NWP1_U + X_test_cpy.NWP2_U + X_test_cpy.NWP3_U)/3\n",
    "    X_test_cpy['V'] = (X_test_cpy.NWP1_V + X_test_cpy.NWP2_V + X_test_cpy.NWP3_V)/3\n",
    "    X_test_cpy['T'] = (X_test_cpy.NWP1_T + X_test_cpy.NWP3_T)/2\n",
    "    X_test_cpy['CLCT'] = X_test_cpy.NWP4_CLCT\n",
    " \n",
    "    X_train_cpy = X_train_cpy[['ID','Time','U','V','T','CLCT']]\n",
    "    X_test_cpy = X_test_cpy[['ID','Time','U','V','T','CLCT']]\n",
    "\n",
    "    ####### Limpieza de outliers ########\n",
    "    X_train_cpy.loc[X_train_cpy['CLCT'] < 0, 'CLCT'] = 0.0\n",
    "    X_test_cpy.loc[X_test_cpy['CLCT'] < 0, 'CLCT'] = 0.0\n",
    "    \n",
    "    # añadir columna Production\n",
    "    X_train_cpy['Production'] = list(Y_train_cpy['Production'])\n",
    "    \n",
    "    # calcular módulo velocidad del viento\n",
    "    X_train_cpy['vel'] = X_train_cpy.apply(get_wind_velmod, axis=1)\n",
    "    \n",
    "    X_ = X_train_cpy[['vel','Production']]\n",
    "    \n",
    "    # parámetros por granja\n",
    "    if WF == 'WF1':\n",
    "        top_frac_max = 0.97\n",
    "        sparse_bin_width = 0.1\n",
    "        frac_std = 0.5\n",
    "        threshold_type = 'scalar'\n",
    "        bottom_max = 6.\n",
    "    elif WF == \"WF2\":\n",
    "        top_frac_max = 0.97\n",
    "        sparse_bin_width = 0.1\n",
    "        frac_std = 0.7\n",
    "        threshold_type = 'scalar'\n",
    "        bottom_max = 5.3\n",
    "    elif WF == \"WF3\":\n",
    "        top_frac_max = 0.97\n",
    "        sparse_bin_width = 0.1\n",
    "        frac_std = 0.55\n",
    "        threshold_type = 'scalar'\n",
    "        bottom_max = 5.\n",
    "    elif WF == \"WF4\":\n",
    "        top_frac_max = 0.97\n",
    "        sparse_bin_width = 0.1\n",
    "        frac_std = 0.95\n",
    "        threshold_type = 'scalar'\n",
    "        bottom_max = 5.3\n",
    "    elif WF == \"WF5\":\n",
    "        top_frac_max = 0.97\n",
    "        sparse_bin_width = 0.1\n",
    "        frac_std = 0.55\n",
    "        threshold_type = 'scalar'\n",
    "        bottom_max = 5.\n",
    "    else:\n",
    "        top_frac_max = 0.97\n",
    "        sparse_bin_width = 0.05\n",
    "        frac_std = 0.85\n",
    "        threshold_type = 'scalar'\n",
    "        bottom_max = 5.\n",
    "        \n",
    "    # top-curve stacked outliers\n",
    "    top_stacked = filters.window_range_flag(\n",
    "        X_.Production, top_frac_max * X_.Production.max(), X_.Production.max(), X_.vel, 12.5, 2000.\n",
    "    )\n",
    "    \n",
    "    # sparse outliers\n",
    "    max_bin = 0.97*X_['Production'].max()\n",
    "    sparse_outliers = filters.bin_filter(\n",
    "        X_.Production, X_.vel, sparse_bin_width, frac_std * X_.Production.std(), 'median', 0.1, max_bin, threshold_type, 'all'\n",
    "    )\n",
    "    \n",
    "    # bottom-curve stacked outliers\n",
    "    bottom_stacked = filters.window_range_flag(X_.vel, bottom_max, 40, X_.Production, 0.05, 2000.)\n",
    "    \n",
    "    # deleting outliers\n",
    "    X_.vel = X_.vel[(~top_stacked) & (~sparse_outliers) & (~bottom_stacked)]\n",
    "    X_.Production = X_.Production[(~top_stacked) & (~sparse_outliers) & (~bottom_stacked)]\n",
    "    plot_flagged_pc(X_.vel, X_.Production, np.repeat('True', len(X_.vel)), 0.7)\n",
    "    \n",
    "    # seleccionar filas correspondientes a los no-outliers\n",
    "    X_train_cpy = X_train_cpy.loc[X_train_cpy['vel'].isin(X_.vel)]\n",
    "    Y_train_cpy = Y_train_cpy.loc[Y_train_cpy['ID'].isin(X_train_cpy['ID'])]\n",
    "    Y_train_cpy = Y_train_cpy['Production']\n",
    "\n",
    "    \n",
    "    # Eliminamos las columnas 'vel' y 'Production'\n",
    "    del X_train_cpy['vel']\n",
    "    del X_train_cpy['Production']\n",
    "    ###################################\n",
    "\n",
    "    ## Feature engineering pipeline #####\n",
    "    feat_adder = dtr.NewFeaturesAdder(add_time_feat=True, add_cycl_feat=True, add_inv_T=True,add_interactions=True)\n",
    "    \n",
    "    drop_lst = []\n",
    "    if feat_adder.get_params().get('add_cycl_feat'):\n",
    "        if feat_adder.get_params().get('add_inv_T'):\n",
    "            drop_lst = [\"ID\", \"Time\", \"U\", \"V\", \"wdir\", \"hour\", \"month\", \"T\"]\n",
    "        else:\n",
    "            drop_lst = [\"ID\", \"Time\", \"U\", \"V\", \"wdir\", \"hour\", \"month\"]\n",
    "    else:\n",
    "        if feat_adder.get_params().get('add_inv_T'):\n",
    "            drop_lst = [\"ID\", \"Time\", \"U\", \"V\", \"T\"]\n",
    "        else:\n",
    "            drop_lst = [\"ID\", \"Time\", \"U\", \"V\"]\n",
    "    \n",
    "    pre_process = ColumnTransformer(remainder = 'passthrough',\n",
    "                                    transformers = [(\n",
    "                                        'drop_columns', 'drop', drop_lst)\n",
    "                                    ])\n",
    "    \n",
    "    # transforming target because of its skweed distribution.\n",
    "    tt = Pipeline(steps=[\n",
    "        ('powertransformer', PowerTransformer(method='yeo-johnson', standardize=True)), \n",
    "    ])\n",
    "    \n",
    "    feat_eng_pipeline = Pipeline(steps=[\n",
    "        ('attr_adder', feat_adder), \n",
    "        ('pre_processing', pre_process),\n",
    "        ('powertransformer', PowerTransformer(method='yeo-johnson', standardize=True)),   \n",
    "    ])\n",
    "    \n",
    "    \n",
    "    X_train_pped = feat_eng_pipeline.fit_transform(X_train_cpy)\n",
    "    X_test_pped = feat_eng_pipeline.transform(X_test_cpy)\n",
    "    \n",
    "    #### \n",
    "    # Feature selection\n",
    "    feature_names = X_train_cpy.drop(drop_lst, axis=1).columns\n",
    "    selec_k_best = SelectKBest(mutual_info_regression, k=1)        \n",
    "    \n",
    "    # make scorers\n",
    "    cape_scorer = make_scorer(metric.get_cape, greater_is_better=False)\n",
    "    \n",
    "    pipeline = Pipeline([(\"univariate_sel\", selec_k_best), (\"mars\", Earth(feature_importance_type='gcv'))] )\n",
    "    \n",
    "    ## Modeling: MARS using py-earth ######\n",
    "    param_grid = {'mars__max_degree': [3,4,5], \n",
    "                  'mars__allow_linear': [False, True], \n",
    "                  'mars__penalty': [0.,1.,2.,3.,4.,5.,6.],\n",
    "                  'univariate_sel__k': [1,2],\n",
    "                  }\n",
    "    \n",
    "    # Cross validation and hyper-parameter tunning with grid seach\n",
    "    n_splits = 7\n",
    "    tscv = TimeSeriesSplit(n_splits)\n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline, \n",
    "        param_grid, \n",
    "        cv=tscv,\n",
    "        scoring=cape_scorer,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train_pped, Y_train_cpy)\n",
    "    \n",
    "    # Feature selection with the best k value obtanined in Grid Search\n",
    "    selec_k_best = SelectKBest(mutual_info_regression, k=grid_search.best_params_['univariate_sel__k'])\n",
    "    selec_k_best.fit(X_train_pped, Y_train_cpy.to_numpy().reshape(-1))\n",
    "    X_train_pped = selec_k_best.transform(X_train_pped)\n",
    "    X_test_pped = selec_k_best.transform(X_test_pped)\n",
    "    \n",
    "    mask =selec_k_best.get_support() #list of booleans\n",
    "    selected_feat = [] \n",
    "\n",
    "    for bool, feature in zip(mask, feature_names):\n",
    "        if bool:\n",
    "            selected_feat.append(feature)  \n",
    "\n",
    "        # Re-training without CV, using the best param#eters obtained by CV\n",
    "        mars = Earth(max_degree=grid_search.best_params_['mars__max_degree'],\n",
    "                 allow_linear=grid_search.best_params_['mars__allow_linear'],\n",
    "                 penalty=grid_search.best_params_['mars__penalty']\n",
    "                )\n",
    "\n",
    "    mars_ttreg = TransformedTargetRegressor(regressor=mars, \n",
    "                                   transformer=tt, \n",
    "                                   check_inverse=False)\n",
    "    mars_ttreg.fit(X_train_pped, Y_train_cpy)\n",
    "    \n",
    "\n",
    "    # Re-training without CV, using the best param#eters obtained by CV\n",
    "    mars = Earth(max_degree=grid_search.best_params_['mars__max_degree'],\n",
    "             allow_linear=grid_search.best_params_['mars__allow_linear'],\n",
    "            penalty=grid_search.best_params_['mars__penalty']\n",
    "            )\n",
    "\n",
    "    mars_ttreg = TransformedTargetRegressor(regressor=mars, \n",
    "                                   transformer=tt, \n",
    "                                   check_inverse=False)\n",
    "\n",
    "    mars_ttreg = TransformedTargetRegressor(regressor=mars, \n",
    "                                   transformer=tt, \n",
    "                                   check_inverse=False)\n",
    "    mars_ttreg.fit(X_train_pped, Y_train_cpy)\n",
    "    \n",
    "    # predicting on test data\n",
    "    predictions = mars_ttreg.predict(X_test_pped)\n",
    "\n",
    "    # build prediction matrix (ID,Production)\n",
    "    pred_matrix = np.stack((np.array(ID_test).astype(int), predictions), axis=-1)\n",
    "    df_pred = pd.DataFrame(data=pred_matrix.reshape(-1,2), columns=['ID','Production'])\n",
    "    \n",
    "    # corregimos valores negativos\n",
    "    df_pred.loc[df_pred['Production'] < 0, 'Production'] = 0.0\n",
    "\n",
    "    # añadimos las predicciones al dataframe final que contendrá las de todas las WF\n",
    "    submission_df = submission_df.append(df_pred, ignore_index=True)\n",
    "    print('Best hyperparameter values: ',grid_search.best_params_)\n",
    "    print('CAPE:', -grid_search.best_score_)\n",
    "    print('Selected Features: ', selected_feat)\n",
    "    print('Predictions for {} has been added to submission_df'.format(WF))\n",
    "    print('---------')\n",
    "    \n",
    "# generamos fichero csv para el submission\n",
    "submission_df.to_csv(\"../../../../TFM/models/mars_all_feat.csv\", index=False, sep=\",\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-04 18:01:30,913 - kedro.io.data_catalog - INFO - Loading data from `X_train_raw` (CSVDataSet)...\n",
      "2020-10-04 18:01:31,291 - kedro.io.data_catalog - INFO - Loading data from `X_test_raw` (CSVDataSet)...\n",
      "2020-10-04 18:01:31,570 - kedro.io.data_catalog - INFO - Loading data from `y_train_raw` (CSVDataSet)...\n"
     ]
    }
   ],
   "source": [
    "X_train_raw = ctx.catalog.load(\"X_train_raw\")\n",
    "X_test_raw = ctx.catalog.load(\"X_test_raw\")\n",
    "y_train_raw = ctx.catalog.load(\"y_train_raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "SavedModel file does not exist at: ../../data/06_models/WF1/ANN.h5/{saved_model.pbtxt|saved_model.pb}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-131-cf073701ae4c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msource_folder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"../../data/06_models/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"../../data/06_models/WF1/ANN.h5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\quark\\.conda\\envs\\tfm-env\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\save.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[0;32m    184\u001b[0m     \u001b[0mfilepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpath_to_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m       \u001b[0mloader_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_saved_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    187\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0msaved_model_load\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\quark\\.conda\\envs\\tfm-env\\lib\\site-packages\\tensorflow\\python\\saved_model\\loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model\u001b[1;34m(export_dir)\u001b[0m\n\u001b[0;32m    111\u001b[0m                   (export_dir,\n\u001b[0;32m    112\u001b[0m                    \u001b[0mconstants\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSAVED_MODEL_FILENAME_PBTXT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m                    constants.SAVED_MODEL_FILENAME_PB))\n\u001b[0m\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: SavedModel file does not exist at: ../../data/06_models/WF1/ANN.h5/{saved_model.pbtxt|saved_model.pb}"
     ]
    }
   ],
   "source": [
    "source_folder = \"../../data/06_models/\"\n",
    "model = tf.keras.models.load_model(\"../../data/06_models/WF1/ANN.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/06_models/'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"../../data/06_models/WF1/ANN.h5\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
